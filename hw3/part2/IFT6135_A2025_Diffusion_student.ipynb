{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "RJIsW-Wskuzm",
      "metadata": {
        "id": "RJIsW-Wskuzm"
      },
      "source": [
        "# IFT6135-A2025\n",
        "\n",
        "# Question 2: Diffusion Practical\n",
        "\n",
        "You must fill in your answers to various questions in the python file and put it next to this notebook. Name the first file as `diffussion_solution.py` and the second file as `CFGdiffusion_solution.py`. In this notebook we will import the functions from those files and do the training. \n",
        "\n",
        "Only edit the functions specified in the PDF (and wherever marked â€“ `# WRITE CODE HERE`). Do not change definitions or edit the rest of the template, else the autograder will not work.\n",
        "\n",
        "**Make sure you request a GPU runtime!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tzOCo7WPkof2",
      "metadata": {
        "id": "tzOCo7WPkof2"
      },
      "source": [
        "# Diffusion Basics\n",
        "\n",
        "Diffusion models are a new and recent class of generative models that rely on a forward diffusion process and a backward denoising process. The forward diffusion process adds a little bit of noise at each step, thus making the input image progressively noisier. On the other hand, the aim of the backward process is to denoise at each step, and is supposed to reverse the effect of the forward process. In this setup, only the backward process is parameterized by a learnable model while the forward process converges to something known, like $\\mathcal{N}(0, I)$. If the learning is done correctly and the backward process works well enough, it would ideally be able to progressively remove noise from complete noise and lead to a sample from the data distribution.\n",
        "\n",
        "Now, lets try to formalize this. Suppose our data samples come from the distribution $q(x_0)$. The forward distribution can be parameterized as\n",
        "\n",
        "\\begin{align*}\n",
        "q(x_t | x_{t-1}) = \\mathcal{N}(\\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)\n",
        "\\end{align*}\n",
        "\n",
        "where if we consider a finite number of timesteps T, then\n",
        "\n",
        "\\begin{align*}\n",
        "q(x_{1:T} | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})\n",
        "\\end{align*}\n",
        "\n",
        "where $\\beta_t$ are the hyperparameters that govern how quickly structure is destroyed in the forward process. One can see this as progressively adding more noise to the input data. One benefit of considering a gaussian distribution above is that it leads to a very nice property that the distribution $q(x_t | x_0)$ becomes known and tractable, so one is able to directly generate a sample from any point in the forward trajectory. In particular, with a bit of algebra, one can obtain\n",
        "\n",
        "\\begin{align*}\n",
        "q(x_t | x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\n",
        "\\end{align*}\n",
        "\n",
        "where $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_t$ and $\\alpha_t = 1 - \\beta_t$. And ideally we want to set $\\beta_t$'s such that $q(x_T | x_0) \\approx \\mathcal{N}(0, I)$.\n",
        "\n",
        "So far, we have obtained the forward structure-destroying diffusion process as well as how to sample directly from any point in the forward process conditioned on the initial conditions. Now, we want to learn a reverse process that takes us back from a noisy sample to something that has less noise. We do this by parameterizing this distribution with a Neural Network like\n",
        "\n",
        "\\begin{align*}\n",
        "  p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t)\n",
        "\\end{align*}\n",
        "\n",
        "where we consider $p(x_T)$ as just $\\mathcal{N}(0, I)$ since at the end of the forward diffusion we are approximating that. Further, we also assume that each conditional above is parameterized as a gaussian distribution, that is\n",
        "\n",
        "\\begin{align*}\n",
        "  p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(\\mu_\\theta(x_t, t), \\tilde{\\beta}_t)\n",
        "\\end{align*}\n",
        "\n",
        "where $\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$ which is because after doing some algebra, we can find that\n",
        "\n",
        "\\begin{align*}\n",
        "  q(x_{t-1} | x_t, x_0) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t)\n",
        "\\end{align*}\n",
        "\n",
        "where $\\tilde{\\mu}_t = \\frac{\\sqrt{\\alpha_t} ( 1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}x_0$.\n",
        "\n",
        "This particular parameterization of $q(x_{t-1} | x_t, x_0)$ follows through by considering the Bayes rule combined with some algebra. Since the backward learned process is aimed to approximate the true backward process $q(x_{t-1} | x_t, x_0)$, it boils down to matching the means $\\tilde{\\mu}_t$ with $\\mu_\\theta(x_t)$ as the variances are kept the same by design. A little more algebraic manipulation and reparameterization tricks lead us to\n",
        "\n",
        "\\begin{align*}\n",
        "  \\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t\\right)\n",
        "\\end{align*}\n",
        "\n",
        "To match this, we need $\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t\\right)\n",
        "$\n",
        "\n",
        "Given this formulation for $\\mu_\\theta$, we can use reparameterization to get $x_{t-1}$ from $x_t$ as\n",
        "\n",
        "\\begin{align*}\n",
        "  x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t\\right) + \\tilde{\\beta}_t \\epsilon\n",
        "\\end{align*}\n",
        "\n",
        "Thus, instead of parameterizing $\\mu_\\theta$ using a Neural Network, one can parameterize $\\epsilon_t$ using the network so that we can do the backward diffusion as\n",
        "\n",
        "\\begin{align*}\n",
        "  x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)\\right) + \\tilde{\\beta}_t \\epsilon\n",
        "\\end{align*}\n",
        "\n",
        "In short, thus, we get a forward diffusion process parameterized as $q(x_t | x_{t-1} = \\mathcal{N}(\\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$ where one can sample $q(x_t | x_0)$ in one-shot in closed form without having to go through $t = 1, ..., t-1$. On the other hand, the backward process parameterizes the noise at time-step $t$, that is $\\epsilon_\\theta(x_t, t)$ which can be used to run the backward process as $p(x_{t-1} | x_t) = \\mathcal{N}(\\frac{1}{\\sqrt{\\alpha}_t} \\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} }\\epsilon_\\theta(x_t, t)\\right), \\tilde{\\beta}_t I)$.\n",
        "\n",
        "And learning of this model leads to a simple objective function, which can be defined as\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{t\\sim \\mathcal{U}(1,T), x_0, \\epsilon_t} \\left[|| \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t, t) ||^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "For our settings, instead of the $L_2$ loss, we will use the huber loss, which is $L_1$ loss but near the origin, acts as an $L_2$ loss. For details, refer [here](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html). For more details about diffusion models, please refer to the [DDPM paper](https://arxiv.org/abs/2006.11239) and the related [blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f2d714",
      "metadata": {
        "id": "a1f2d714"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "from torchvision import transforms\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def fix_experiment_seed(seed=0):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fix_experiment_seed()\n",
        "\n",
        "results_folder = Path(\"./results_diffusion\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uu-S0tNpk56T",
      "metadata": {
        "id": "Uu-S0tNpk56T"
      },
      "source": [
        "## Set up the hyperparameters\n",
        "- Batch Size\n",
        "- Latent Dimensionality\n",
        "- Learning Rate\n",
        "- Diffusion timesteps: $T$\n",
        "- Starting variance: $\\beta_1$\n",
        "- Ending variance: $\\beta_T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rcZzhdIHk6et",
      "metadata": {
        "id": "rcZzhdIHk6et"
      },
      "outputs": [],
      "source": [
        "# Training Hyperparameters\n",
        "batch_size = 256  # Batch Size\n",
        "z_dim = 32        # Latent Dimensionality\n",
        "lr = 1e-4         # Learning Rate\n",
        "\n",
        "# Hyperparameters taken from Ho et. al for noise scheduling\n",
        "T = 1000            # Diffusion Timesteps\n",
        "beta_start = 0.0001 # Starting variance\n",
        "beta_end = 0.02     # Ending variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Djm92b3dlAQ4",
      "metadata": {
        "id": "Djm92b3dlAQ4"
      },
      "outputs": [],
      "source": [
        "# Define Dataset Statistics\n",
        "image_size = 32\n",
        "input_channels = 1\n",
        "\n",
        "# Resize and Normalize the Data\n",
        "transform = Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# Helper Functions\n",
        "def show_image(image, nrow=8):\n",
        "  # Input: image\n",
        "  # Displays the image using matplotlib\n",
        "  grid_img = make_grid(image.detach().cpu(), nrow=nrow, padding=0)\n",
        "  plt.imshow(grid_img.permute(1, 2, 0))\n",
        "  plt.axis('off')\n",
        "\n",
        "def transforms_examples(examples):\n",
        "  if \"image\" in examples:\n",
        "     examples[\"pixel_values\"] = [transform(image) for image in examples[\"image\"]]\n",
        "     del examples[\"image\"]\n",
        "  else:\n",
        "     examples[\"pixel_values\"] = [transform(image) for image in examples[\"img\"]]\n",
        "     del examples[\"img\"]\n",
        "\n",
        "  return examples\n",
        "\n",
        "\n",
        "def get_dataloaders():\n",
        "    dataset = load_dataset(\"mnist\", cache_dir='./data')\n",
        "    transformed_dataset = dataset.with_transform(transforms_examples)\n",
        "    \n",
        "    train_dataloader = DataLoader(\n",
        "        transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True, drop_last=True\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        transformed_dataset[\"test\"], batch_size=batch_size, shuffle=False, drop_last=False\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YHQL39dhlDY-",
      "metadata": {
        "id": "YHQL39dhlDY-"
      },
      "source": [
        "## Visualize the Data\n",
        "\n",
        "Let's visualize what our data actually looks like! We are using the [CIFAR-10](https://huggingface.co/datasets/cifar10) dataset. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 different classes. It has a training set of 50,000 examples and a test set of 10,000 examples. The dataset is divided into five training batches and one test batch, each with 10,000 images. Please note that you don't need to download the dataset yourself as the code we provided downloads the dataset for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "maXVk2OPlEeY",
      "metadata": {
        "id": "maXVk2OPlEeY"
      },
      "outputs": [],
      "source": [
        "# Visualize the Dataset\n",
        "def visualize():\n",
        "  train_dataloader, _ = get_dataloaders()\n",
        "  batch = next(iter(train_dataloader))\n",
        "  print(batch['pixel_values'].shape)\n",
        "\n",
        "  save_image((batch['pixel_values'] + 1.) * 0.5, './results_diffusion/orig.png')\n",
        "  show_image((batch['pixel_values'] + 1.) * 0.5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uAPXFtg5xCrO",
      "metadata": {
        "id": "uAPXFtg5xCrO"
      },
      "source": [
        "## Helper Functions / Building Blocks\n",
        "\n",
        "Here we provide some helper functions and building blocks that will allow us to create the U-Net network that parameterizes the backward noise prediction network in diffusion models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a581f5a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.channels = channels        \n",
        "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape[-1]\n",
        "        x = x.view(-1, self.channels, size * size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "        return attention_value.swapaxes(2, 1).view(-1, self.channels, size, size)\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
        "        super().__init__()\n",
        "        self.residual = residual\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            return F.gelu(x + self.double_conv(x))\n",
        "        else:\n",
        "            return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.maxpool_conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, c_in=3, c_out=3, time_dim=128, remove_deep_conv=False):\n",
        "        super().__init__()\n",
        "        self.time_dim = time_dim\n",
        "        self.remove_deep_conv = remove_deep_conv\n",
        "        self.inc = DoubleConv(c_in, 32)\n",
        "        self.down1 = Down(32, 64)\n",
        "        #self.sa1 = SelfAttention(64)\n",
        "        self.down2 = Down(64, 128)\n",
        "        #self.sa2 = SelfAttention(128)\n",
        "        self.down3 = Down(128, 128)\n",
        "        #self.sa3 = SelfAttention(128)\n",
        "\n",
        "\n",
        "        if remove_deep_conv:\n",
        "            self.bot1 = DoubleConv(128, 128)\n",
        "            self.bot3 = DoubleConv(128, 128)\n",
        "        else:\n",
        "            self.bot1 = DoubleConv(128, 256)\n",
        "            self.bot2 = DoubleConv(256, 256)\n",
        "            self.bot3 = DoubleConv(256, 128)\n",
        "\n",
        "        self.up1 = Up(256, 64)\n",
        "        #self.sa4 = SelfAttention(64)\n",
        "        self.up2 = Up(128, 32)\n",
        "        #self.sa5 = SelfAttention(32)\n",
        "        self.up3 = Up(64, 32)\n",
        "        #self.sa6 = SelfAttention(32)\n",
        "        self.outc = nn.Conv2d(32, c_out, kernel_size=1)\n",
        "\n",
        "    def pos_encoding(self, t, channels):\n",
        "        inv_freq = 1.0 / (\n",
        "            10000\n",
        "            ** (torch.arange(0, channels, 2, device=t.device).float() / channels)\n",
        "        )\n",
        "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
        "        return pos_enc\n",
        "\n",
        "    def unet_forwad(self, x, t):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1, t)\n",
        "        #x2 = self.sa1(x2)\n",
        "        x3 = self.down2(x2, t)\n",
        "        #x3 = self.sa2(x3)\n",
        "        x4 = self.down3(x3, t)\n",
        "        #x4 = self.sa3(x4)\n",
        "\n",
        "        x4 = self.bot1(x4)\n",
        "        if not self.remove_deep_conv:\n",
        "            x4 = self.bot2(x4)\n",
        "        x4 = self.bot3(x4)\n",
        "\n",
        "        x = self.up1(x4, x3, t)\n",
        "        #x = self.sa4(x)\n",
        "        x = self.up2(x, x2, t)\n",
        "        #x = self.sa5(x)\n",
        "        x = self.up3(x, x1, t)\n",
        "        #x = self.sa6(x)\n",
        "        output = self.outc(x)\n",
        "        return output\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        t = t.unsqueeze(-1)\n",
        "        t = self.pos_encoding(t, self.time_dim)\n",
        "        return self.unet_forwad(x, t)\n",
        "\n",
        "\n",
        "def load_weights(eps_model, PATH):\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(PATH, weights_only=True)\n",
        "        eps_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        eps_model.eval()\n",
        "        print('UNet model loaded (in eval mode)!') \n",
        "        return eps_model\n",
        "        \n",
        "    except:\n",
        "        print('No weights to load') \n",
        "        return eps_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30368b2",
      "metadata": {
        "id": "a30368b2"
      },
      "source": [
        "We define a helper function *extract* which takes as input a tensor *a* and an index tesor *t* and returns another tensor where the $i^{th}$ element of this new tensor corresponds to $a[t[i]]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B_k8rLDFzJ1C",
      "metadata": {
        "id": "B_k8rLDFzJ1C"
      },
      "outputs": [],
      "source": [
        "def extract(a, t, x_shape):\n",
        "  # Takes a data tensor a and an index tensor t, and returns a new tensor\n",
        "  # whose i^th element is just a[t[i]]. Note that this will be useful when\n",
        "  # we would want to choose the alphas or betas corresponding to different\n",
        "  # indices t's in a batched manner without for loops.\n",
        "  # Inputs:\n",
        "  #   a: Tensor, generally of shape (batch_size,)\n",
        "  #   t: Tensor, generally of shape (batch_size,)\n",
        "  #   x_shape: Shape of the data, generally (batch_size, 3, 32, 32)\n",
        "  # Returns:\n",
        "  #   out: Tensor of shape (batch_size, 1, 1, 1) generally, the number of 1s are\n",
        "  #         determined by the number of dimensions in x_shape.\n",
        "  #         out[i] contains a[t[i]]\n",
        "\n",
        "  batch_size = t.shape[0]\n",
        "  out = a.gather(-1, t.cpu())\n",
        "  return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2EnfxY7ZFFAQ",
      "metadata": {
        "id": "2EnfxY7ZFFAQ"
      },
      "source": [
        "Now, we define the different coefficients that are required in the diffusion process. In particular, we need to define the following tensors, all of which are of size $(T,)$. Also note that we are using indexing starting from 1 here, in the code all the variables with $t=1$ are set at position 0. \n",
        "\n",
        "- betas: Contains $\\beta_t$ from the linear scheduling between $\\beta_1$ and $\\beta_T$, sampled over $T$ intervals\n",
        "- alphas: Contains $\\alpha_t = 1-\\beta_t$\n",
        "- sqrt_recip_alphas: Contains $\\frac{1.}{\\sqrt{{\\alpha}_t}}$\n",
        "- alphas_cumprod: Contains $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n",
        "- sqrt_alphas_cumprod: Contains $\\sqrt{\\bar{\\alpha}_t}$\n",
        "- sqrt_one_minus_alphas_cumprod: Contains $\\sqrt{1 - \\bar{\\alpha}_t}$\n",
        "- alphas_cumprod_prev: Right shift $\\bar{\\alpha}_t$; thus contains $\\prod_{i=1}^{t-1} \\alpha_i$ with the first element as 1.\n",
        "- posterior_variance: Contains $\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d751df2",
      "metadata": {
        "id": "5d751df2"
      },
      "outputs": [],
      "source": [
        "def alphas_betas_sequences_helper(beta_start, beta_end, T):\n",
        "    def linear_beta_schedule(beta_start, beta_end, timesteps):\n",
        "        return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "    betas = linear_beta_schedule(beta_start, beta_end, T)                           # Define the linear beta schedule\n",
        "    alphas = 1. - betas                                                             # Compute the alphas as 1 - betas\n",
        "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)                                    # Returns 1/square_root(\\alpha_t)\n",
        "    alphas_cumprod = torch.cumprod(alphas, axis=0)                                  # Compute product of alphas up to index t, \\bar{\\alpha}\n",
        "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)                                # Returns sqaure_root(\\bar{\\alpha}_t)\n",
        "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)                 # Returns square_root(1 - \\bar{\\alpha}_t)\n",
        "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)             # Right shifts \\bar{\\alpha}_t; with first element as 1.\n",
        "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # Contains the posterior variances $\\tilde{\\beta}_t$\n",
        "\n",
        "    return betas, alphas, sqrt_recip_alphas, alphas_cumprod, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, alphas_cumprod_prev, posterior_variance\n",
        "\n",
        "betas, alpha, sqrt_recip_alphas, alphas_cumprod, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, alphas_cumprod_prev, posterior_variance = alphas_betas_sequences_helper(beta_start, beta_end, T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nJmq5Q0mJkBX",
      "metadata": {
        "id": "nJmq5Q0mJkBX"
      },
      "source": [
        "# Forward Diffusion Process\n",
        "\n",
        "To define the forward diffusion, we need to model the distribution $q(x_t | x_0)$ in a manner that does not go through $t = 1, ..., t-1$. In particular, as defined at the start, we can obtain the distribution as\n",
        "\n",
        "\\begin{align*}\n",
        "  q(x_t | x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\n",
        "\\end{align*}\n",
        "\n",
        "Since you have already computed the required coefficients above, your task is to now complete the function q_sample which takes $(x_0, t)$ as input and returns a sample from $q(x_t | x_0)$ in a batched manner, that is, in parallel for a batch of $x_0$'s and a batch of different timesteps $t$. You can use the extract function provided to get the coefficients at the right timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3752480",
      "metadata": {
        "id": "f3752480"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import q_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82bac28",
      "metadata": {
        "id": "e82bac28"
      },
      "source": [
        "Let's test the forward diffusion process on a particular image sample. We will see that the sample progressively loses all structure and ends up close to completely random noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd64f89",
      "metadata": {
        "id": "6bd64f89"
      },
      "outputs": [],
      "source": [
        "def visualize_diffusion():\n",
        "  train_dataloader, _ = get_dataloaders()\n",
        "  batch = next(iter(train_dataloader))\n",
        "  sample = batch['pixel_values'][3].unsqueeze(0)\n",
        "  coefficients = (\n",
        "      sqrt_alphas_cumprod,\n",
        "      sqrt_one_minus_alphas_cumprod,\n",
        "  )\n",
        "  noisy_images = [sample] + [q_sample(sample, torch.tensor([100 * t + 99]), coefficients) for t in range(10)]\n",
        "  noisy_images = (torch.cat(noisy_images, dim=0) + 1.) * 0.5\n",
        "  show_image(noisy_images.clamp(0., 1.), nrow=11)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  visualize_diffusion()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0SkxPpp-LeDO",
      "metadata": {
        "id": "0SkxPpp-LeDO"
      },
      "source": [
        "# Backward Learned Diffusion Process\n",
        "\n",
        "Now suppose you have access to the model $\\epsilon_\\theta$ in the above description of diffusion models. We know that given a noisy sample $x_t$, one can obtain a slightly denoised version of this sample through the distribution $p_\\theta(x_{t-1} | x_t)$, which in our setup is now defined as\n",
        "\n",
        "\\begin{align*}\n",
        "p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(\\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\\right), \\tilde{\\beta}_t I)\n",
        "\\end{align*}\n",
        "\n",
        "Below, the task is to complete the function p_sample that takes as input the denoising model $\\epsilon_\\theta$, a batched noisy image $x$, a batched time-step $t$ and a scalar $t\\_index$, and it has to return a sample from $p(x_{t-1} | x_t)$. In the case that $t=1$ (or in code, $t=0$), please just return the mode instead of a sample. For doing this if-condition, you can use $t\\_index$ which is just a scalar instead of its batched variant $t$.\n",
        "\n",
        "Note that all the coefficients $\\bar{\\alpha}_t$, etc. are already computed above for all $t$, so use the extract function provided to obtain them at different corresponding timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7628fb3",
      "metadata": {
        "id": "f7628fb3"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import p_sample, p_sample_loop\n",
        "\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    # Returns a sample by running the sampling loop\n",
        "    with torch.no_grad():\n",
        "        return p_sample_loop(\n",
        "            model,\n",
        "            shape=(batch_size, channels, image_size, image_size),\n",
        "            timesteps=T,\n",
        "            T=T,\n",
        "            coefficients=(\n",
        "                betas,\n",
        "                sqrt_one_minus_alphas_cumprod,\n",
        "                sqrt_recip_alphas,\n",
        "                posterior_variance,\n",
        "            ),\n",
        "            noise=None\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J83yuyoppec0",
      "metadata": {
        "id": "J83yuyoppec0"
      },
      "source": [
        "# Define the Loss\n",
        "\n",
        "Now that we have both the forward and the backward diffusion process ready, we need a training criterion. In the introduction, we already saw that the optimization objective for training is to minimize:\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{t\\sim \\mathcal{U}(1,T), x_0, \\epsilon_t} \\left[|| \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t, t) ||^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "This boils down to\n",
        "\n",
        "- Generating some gaussian noise from $\\mathcal{N}(0, I)$.\n",
        "- Getting the noisy images at time $t$ in a batched, one-shot fashion.\n",
        "- Getting the estimate of noise from the noisy images.\n",
        "- Computing the loss between the estimate of noise and the actual noise.\n",
        "\n",
        "In practice here, we will use the **huber** loss instead of the squared loss; so please implement that. Feel free to use PyTorch's criterion to get the huber loss formulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7725f6cf",
      "metadata": {
        "id": "7725f6cf"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import p_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76FreGeEyDG5",
      "metadata": {
        "id": "76FreGeEyDG5"
      },
      "source": [
        "### Random sampling of time-step\n",
        "\n",
        "Finally, randomly sample time-steps from a uniform distribution over timesteps, and return a tensor of size (batch\\_size,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ezi8ZB4Dxx_b",
      "metadata": {
        "id": "Ezi8ZB4Dxx_b"
      },
      "outputs": [],
      "source": [
        "from diffusion_solution import t_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e4c0fd",
      "metadata": {
        "id": "22e4c0fd"
      },
      "source": [
        "Having defined all the ingredients for **training** and **sampling** from this model, we now define the model itself and the optimizer used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5126e21",
      "metadata": {
        "id": "a5126e21"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = UNet(c_in=1, c_out=1)\n",
        "  model.to(device)\n",
        "  model.device=device\n",
        "\n",
        "  optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7444b0b",
      "metadata": {
        "id": "f7444b0b"
      },
      "source": [
        "Finally, let's start training!\n",
        "Visualization of the samples generated, the original dataset and the reconstructions are saved locally in the notebook! Your task is to just provide sampling of time-steps t, which should be a tensor of size (batch\\_size,) sampled uniformly from $[0, T-1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b12ed1",
      "metadata": {
        "id": "92b12ed1"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_dataloader, test_dataloader = get_dataloaders()\n",
        "  sample(model, image_size=image_size, batch_size=256, channels=input_channels)\n",
        "  for epoch in range(epochs):\n",
        "    with tqdm(train_dataloader, unit=\"batch\", leave=False) as tepoch:\n",
        "      for batch in tepoch:\n",
        "        tepoch.set_description(f\"Epoch: {epoch}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_size = batch[\"pixel_values\"].shape[0]\n",
        "        x = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        t = t_sample(T, batch_size, x.device) # Randomly sample timesteps uniformly from [0, T-1]\n",
        "\n",
        "        loss = p_losses(model, x, t, (sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Sample and Save Generated Images\n",
        "    save_image((x + 1.) * 0.5, './results_diffusion/orig.png')\n",
        "    samples = sample(model, image_size=image_size, batch_size=256, channels=input_channels)\n",
        "    samples = (torch.Tensor(samples[-1]) + 1.0) * 0.5\n",
        "    save_image(samples, f\"./results_diffusion/ddpm_samples_{epoch}.png\")\n",
        "\n",
        "    show_image(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921a9d51",
      "metadata": {},
      "source": [
        "## Classifier Free Guidance\n",
        "\n",
        "### Overview\n",
        "**Classifier-Free Guidance (CFG)** is a technique introduced by *Ho & Salimans (2021)* to improve the quality and controllability of diffusion model sampling **without** relying on a separate classifier network.  \n",
        "Instead of using an external classifier to steer the generation toward a target class, CFG trains **a single model** that can operate in both **conditional** and **unconditional** modes.\n",
        "\n",
        "\n",
        "\n",
        "### Key Idea\n",
        "During training, the model is conditioned on a label (e.g., an MNIST digit) only part of the time.  \n",
        "With probability $p_{\\text{uncond}}$, the label is **dropped** (replaced by `None` or a special token).\n",
        "\n",
        "This encourages the model $\\epsilon_\\theta(x_t, t, y)$ to learn two behaviors:\n",
        "- **Conditional mode:** predict noise given both the noisy image $x_t$ and label $y$  \n",
        "- **Unconditional mode:** predict noise given only $x_t$\n",
        "\n",
        "At inference time, combine unconditional and conditional predictions with a **guidance scale** $s$:\n",
        "\\begin{align*}\n",
        "\\hat{\\epsilon}_\\theta(x_t, t, y)\n",
        "&= \\epsilon_\\theta(x_t, t, \\text{None})\n",
        "\\;+\\; s\\Big(\\epsilon_\\theta(x_t, t, y) - \\epsilon_\\theta(x_t, t, \\text{None})\\Big)\n",
        "\\end{align*}\n",
        "\n",
        "Interpretation of $s$:\n",
        "- $s=0$: purely unconditional generation  \n",
        "- $s=1$: standard conditional sampling  \n",
        "- $s>1$: stronger conditioning (typically more class-consistent, possibly less diverse)\n",
        "\n",
        "\n",
        "\n",
        "### Training Steps\n",
        "1. Sample a clean image $x_0$ and its label $y$.\n",
        "2. Add Gaussian noise to get a noisy sample at time $t$:\n",
        "\\begin{align*}\n",
        "x_t \\;=\\; \\sqrt{\\bar{\\alpha}_t}\\,x_0 \\;+\\; \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon,\n",
        "\\qquad \\epsilon \\sim \\mathcal{N}(0, I)\n",
        "\\end{align*}\n",
        "3. With probability $p_{\\text{uncond}}$, drop the label and train the model to predict $\\epsilon$ from $(x_t, t, y^*)$, where $y^*$ is either $y$ or `None`.\n",
        "4. Optimize a noise-prediction loss (e.g., MSE or Huber) between the predicted and true noise.\n",
        "\n",
        "\n",
        "### Sampling Steps\n",
        "For each reverse diffusion step $t$:\n",
        "\\begin{align*}\n",
        "\\epsilon_{\\text{uncond}} &= \\epsilon_\\theta(x_t, t, \\text{None}) \\\\\n",
        "\\epsilon_{\\text{cond}}   &= \\epsilon_\\theta(x_t, t, y) \\\\\n",
        "\\hat{\\epsilon}_\\theta     &= \\epsilon_{\\text{uncond}} + s\\big(\\epsilon_{\\text{cond}} - \\epsilon_{\\text{uncond}}\\big)\n",
        "\\end{align*}\n",
        "Use $\\hat{\\epsilon}_\\theta$ inside the reverse diffusion update to progressively denoise $x_t \\rightarrow x_0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fbe98b3",
      "metadata": {},
      "source": [
        "### Helper Functions / UNet for Conditional Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cc242c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.channels = channels        \n",
        "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape[-1]\n",
        "        x = x.view(-1, self.channels, size * size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "        return attention_value.swapaxes(2, 1).view(-1, self.channels, size, size)\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
        "        super().__init__()\n",
        "        self.residual = residual\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            return F.gelu(x + self.double_conv(x))\n",
        "        else:\n",
        "            return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=64):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels),\n",
        "        )\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(emb_dim, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x = self.maxpool_conv(x)\n",
        "        if labels is not None:\n",
        "            emb = self.emb_layer(labels)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "            x += emb\n",
        "        return x\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=64):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(emb_dim, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, labels=None):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        if labels is not None:\n",
        "            emb = self.emb_layer(labels)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "            x += emb\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Time-conditioned UNet (unconditional wrt labels).\n",
        "    Keeps your original channels and block structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, c_in=3, c_out=3, c_emb_dim=64, remove_deep_conv=False):\n",
        "        super().__init__()\n",
        "        self.c_emb_dim = c_emb_dim            # embedding dim used in Down/Up emb_layers\n",
        "        self.remove_deep_conv = remove_deep_conv\n",
        "\n",
        "        self.inc = DoubleConv(c_in, 16)\n",
        "        self.down1 = Down(16, 32, emb_dim=c_emb_dim)\n",
        "        self.down2 = Down(32, 64, emb_dim=c_emb_dim)\n",
        "        self.down3 = Down(64, 64, emb_dim=c_emb_dim)\n",
        "\n",
        "        if remove_deep_conv:\n",
        "            self.bot1 = DoubleConv(64, 64)\n",
        "            self.bot3 = DoubleConv(64, 64)\n",
        "        else:\n",
        "            self.bot1 = DoubleConv(64, 128)\n",
        "            self.bot2 = DoubleConv(128, 128)\n",
        "            self.bot3 = DoubleConv(128, 64)\n",
        "\n",
        "        self.up1 = Up(128, 32, emb_dim=c_emb_dim)\n",
        "        self.up2 = Up(64, 16, emb_dim=c_emb_dim)\n",
        "        self.up3 = Up(32, 16, emb_dim=c_emb_dim)\n",
        "        self.outc = nn.Conv2d(16, c_out, kernel_size=1)\n",
        "\n",
        "    # --- sinusoidal time embedding to size c_emb_dim (minimal change) ---\n",
        "    def pos_encoding(self, t, channels):\n",
        "        \"\"\"\n",
        "        t: (B, 1) float\n",
        "        channels: output size (= self.c_emb_dim)\n",
        "        \"\"\"\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=t.device).float() / channels))\n",
        "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
        "        if pos_enc.shape[-1] < channels:  # pad if odd\n",
        "            pad = torch.zeros(pos_enc.shape[0], 1, device=t.device, dtype=pos_enc.dtype)\n",
        "            pos_enc = torch.cat([pos_enc, pad], dim=-1)\n",
        "        return pos_enc\n",
        "\n",
        "    def unet_forwad(self, x, labels):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1, labels)\n",
        "        x3 = self.down2(x2, labels)\n",
        "        x4 = self.down3(x3, labels)\n",
        "\n",
        "        x4 = self.bot1(x4)\n",
        "        if not self.remove_deep_conv:\n",
        "            x4 = self.bot2(x4)\n",
        "        x4 = self.bot3(x4)\n",
        "\n",
        "        x = self.up1(x4, x3, labels)\n",
        "        x = self.up2(x, x2, labels)\n",
        "        x = self.up3(x, x1, labels)\n",
        "        output = self.outc(x)\n",
        "        return output\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        x: (B,C,H,W)\n",
        "        t: (B,) or (B,1) integer/float timesteps\n",
        "        \"\"\"\n",
        "        if t.dim() == 1:\n",
        "            t = t.unsqueeze(-1)\n",
        "        t = t.float()\n",
        "        # Produce time embedding of size self.c_emb_dim\n",
        "        t_emb = self.pos_encoding(t, self.c_emb_dim)  # (B, c_emb_dim)\n",
        "        return self.unet_forwad(x, t_emb)\n",
        "\n",
        "\n",
        "class UNet_conditional(UNet):\n",
        "    \"\"\"\n",
        "    Conditional UNet: adds class-label conditioning on top of time conditioning.\n",
        "    Keeps the same internal block APIs; just sums label embedding with time embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, c_in=3, c_out=3, c_emb_dim=64, num_classes=None, **kwargs):\n",
        "        super().__init__(c_in, c_out, c_emb_dim, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        if num_classes is not None:\n",
        "            self.label_emb = nn.Embedding(num_classes, c_emb_dim)\n",
        "\n",
        "    def forward(self, x, t, y=None):\n",
        "        \"\"\"\n",
        "        x: (B,C,H,W)\n",
        "        t: (B,) or (B,1) timesteps\n",
        "        y: (B,) class indices or None (for unconditional branch in CFG)\n",
        "        \"\"\"\n",
        "        if t.dim() == 1:\n",
        "            t = t.unsqueeze(-1)\n",
        "        t = t.float()\n",
        "\n",
        "        # time embedding (size = c_emb_dim)\n",
        "        t_emb = self.pos_encoding(t, self.c_emb_dim)\n",
        "\n",
        "        # label embedding (or zeros), same size so we can sum\n",
        "        if (y is not None) and (self.num_classes is not None):\n",
        "            y_emb = self.label_emb(y)\n",
        "        else:\n",
        "            y_emb = torch.zeros_like(t_emb)\n",
        "\n",
        "        cond = t_emb + y_emb  # sum keeps it minimal and effective\n",
        "        return self.unet_forwad(x, cond)\n",
        "\n",
        "\n",
        "def load_weights(eps_model, PATH):\n",
        "    try:\n",
        "        checkpoint = torch.load(PATH, map_location=\"cpu\")\n",
        "        if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
        "            eps_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        else:\n",
        "            eps_model.load_state_dict(checkpoint)\n",
        "        eps_model.eval()\n",
        "        print('UNet model loaded (in eval mode)!') \n",
        "        return eps_model\n",
        "    except Exception as e:\n",
        "        print(f'No weights to load ({e})') \n",
        "        return eps_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80be798f",
      "metadata": {},
      "source": [
        "# Forward Diffusion Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9261efb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from CFGdiffusion_solution import q_sample\n",
        "\n",
        "def visualize_diffusion():\n",
        "  train_dataloader, _ = get_dataloaders()\n",
        "  batch = next(iter(train_dataloader))\n",
        "  sample = batch['pixel_values'][3].unsqueeze(0)\n",
        "  coefficients = (\n",
        "      sqrt_alphas_cumprod,\n",
        "      sqrt_one_minus_alphas_cumprod,\n",
        "  )\n",
        "  noisy_images = [sample] + [q_sample(sample, torch.tensor([100 * t + 99]), coefficients) for t in range(10)]\n",
        "  noisy_images = (torch.cat(noisy_images, dim=0) + 1.) * 0.5\n",
        "  show_image(noisy_images.clamp(0., 1.), nrow=11)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  visualize_diffusion()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc61eaa1",
      "metadata": {},
      "source": [
        "# Backward Learned Diffusion Process with Conditional Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cce9f03",
      "metadata": {},
      "outputs": [],
      "source": [
        "from CFGdiffusion_solution import  p_sample, p_sample_loop\n",
        "\n",
        "def sample_cfg(model, image_size, batch_size, channels, y, guidance_scale):\n",
        "    with torch.no_grad():\n",
        "        return p_sample_loop(\n",
        "            model,\n",
        "            shape=(batch_size, channels, image_size, image_size),\n",
        "            timesteps=T,\n",
        "            T=T,\n",
        "            coefficients=(\n",
        "                betas,\n",
        "                sqrt_one_minus_alphas_cumprod,\n",
        "                sqrt_recip_alphas,\n",
        "                posterior_variance,\n",
        "            ),\n",
        "            y=y,\n",
        "            guidance_scale=guidance_scale,\n",
        "            noise=None\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ba8a7e",
      "metadata": {},
      "source": [
        "## Defining Loss and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5648235a",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = UNet_conditional(c_in=1, c_out=1, num_classes=10)\n",
        "model.to(device)\n",
        "model.device=device\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b686602",
      "metadata": {},
      "outputs": [],
      "source": [
        "from CFGdiffusion_solution import  t_sample, p_losses\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 256\n",
        "guidance_scale = 3.5     # try 1.5â€“3.5 for MNIST\n",
        "p_uncond = 0.1           # label dropout prob during training\n",
        "num_classes = 10\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dataloader, test_dataloader = get_dataloaders()\n",
        "\n",
        "    _ = sample_cfg(\n",
        "        model, image_size=image_size, batch_size=16, channels=input_channels,\n",
        "        y=torch.full((16,), 0, dtype=torch.long, device=device),  # digit '0'\n",
        "        guidance_scale=guidance_scale\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        with tqdm(train_dataloader, unit=\"batch\", leave=False) as tepoch:\n",
        "            for batch in tepoch:\n",
        "                tepoch.set_description(f\"Epoch: {epoch}\")\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                x = batch[\"pixel_values\"].to(device)                 \n",
        "                y = batch[\"label\"].to(device).long()                 \n",
        "                B = x.shape[0]\n",
        "\n",
        "                t = t_sample(T, B, x.device)                         \n",
        "                loss = p_losses(\n",
        "                    denoise_model=model,\n",
        "                    x_start=x,\n",
        "                    t=t,\n",
        "                    coefficients=(sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod),\n",
        "                    y=y,\n",
        "                    p_uncond=p_uncond,\n",
        "                    noise=None,\n",
        "                    loss_type=\"l2\",                                  \n",
        "                )\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            n_samples = 256\n",
        "\n",
        "\n",
        "            target_label = 5\n",
        "            y_cond = torch.full((n_samples,), target_label, dtype=torch.long, device=device)\n",
        "\n",
        "            imgs_all_steps = sample_cfg(\n",
        "                model,\n",
        "                image_size=image_size,\n",
        "                batch_size=n_samples,\n",
        "                channels=input_channels,\n",
        "                y=y_cond,\n",
        "                guidance_scale=guidance_scale\n",
        "            )\n",
        "            x_T0 = imgs_all_steps[-1]                            \n",
        "            x_vis = (x_T0 + 1.0) * 0.5                         \n",
        "\n",
        "            save_image((x[:256] + 1.) * 0.5, f'./results_diffusion/orig_e{epoch}.png', nrow=8)\n",
        "            save_image(x_vis, f'./results_diffusion/cfg_label{target_label}_e{epoch}.png', nrow=8)\n",
        "            show_image(x_vis, nrow=8)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
