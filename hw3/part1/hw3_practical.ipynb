{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595854f8",
   "metadata": {},
   "source": [
    "# HW3 Practical: Comparing Generative Paradigms on CIFAR-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8329343",
   "metadata": {},
   "source": [
    "Welcome! This notebook provides the training and evaluation pipeline for the four generative models you will build in `models/`.\n",
    "\n",
    "- Run the setup cells to install dependencies and load CIFAR-10.\n",
    "- Complete the TODOs in the Python modules, verify with Gradescope autograder, then return here to train and evaluate models.\n",
    "- Follow the prompts in each section to log results and save artefacts (sample image grids, metrics, and plots) for inclusion in your PDF report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41506e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install lightweight dependencies (safe to re-run)\n",
    "%pip install --quiet torch-fidelity tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec1c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sidne\\anaconda3\\envs\\IFT6135\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cifar10 import load_cifar10\n",
    "from models.gan import DCGAN\n",
    "from models.vae import ConvVAE\n",
    "from models.pixelcnn import PixelCNN\n",
    "from models.ddpm import DenoiseUNet\n",
    "\n",
    "from torch_fidelity import calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04846bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti\n",
      "Project root: c:\\Users\\sidne\\OneDrive\\Bureau\\Automne2025\\school\\Representationh_L-IFT6135\\homeworks\\hw3\\part1\n"
     ]
    }
   ],
   "source": [
    "# Global configuration\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_ROOT = PROJECT_ROOT\n",
    "ARTIFACT_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca4293",
   "metadata": {},
   "source": [
    "### Experiment presets\n",
    "\n",
    "The dictionaries below define the baseline (\"small\") and scaled (\"medium\") configurations used in the assignment. Baselines train for 30 epochs, and the scaled variants run for 40 epochs so you can contrast added capacity with extra compute. Feel free to explore other values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dcgan': {'small': {'base_channels': 64,\n",
       "   'latent_dim': 128,\n",
       "   'epochs': 30,\n",
       "   'lr': 0.0002},\n",
       "  'medium': {'base_channels': 96,\n",
       "   'latent_dim': 128,\n",
       "   'epochs': 40,\n",
       "   'lr': 0.0002}},\n",
       " 'vae': {'small': {'base_channels': 64,\n",
       "   'latent_dim': 128,\n",
       "   'epochs': 30,\n",
       "   'lr': 0.0002},\n",
       "  'medium': {'base_channels': 96,\n",
       "   'latent_dim': 192,\n",
       "   'epochs': 40,\n",
       "   'lr': 0.0002}},\n",
       " 'pixelcnn': {'small': {'hidden_channels': 64,\n",
       "   'residual_layers': 5,\n",
       "   'epochs': 30,\n",
       "   'lr': 0.0003},\n",
       "  'medium': {'hidden_channels': 96,\n",
       "   'residual_layers': 7,\n",
       "   'epochs': 40,\n",
       "   'lr': 0.0003}},\n",
       " 'ddpm': {'small': {'base_channels': 64,\n",
       "   'time_channels': 256,\n",
       "   'timesteps': 1000,\n",
       "   'epochs': 30,\n",
       "   'lr': 0.0002},\n",
       "  'medium': {'base_channels': 96,\n",
       "   'time_channels': 256,\n",
       "   'timesteps': 750,\n",
       "   'epochs': 40,\n",
       "   'lr': 0.0002}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_CONFIGS = {\n",
    "    \"dcgan\": {\n",
    "        \"small\": {\"base_channels\": 64, \"latent_dim\": 128, \"epochs\": 30, \"lr\": 2e-4},\n",
    "        \"medium\": {\"base_channels\": 96, \"latent_dim\": 128, \"epochs\": 40, \"lr\": 2e-4},\n",
    "    },\n",
    "    \"vae\": {\n",
    "        \"small\": {\"base_channels\": 64, \"latent_dim\": 128, \"epochs\": 30, \"lr\": 2e-4},\n",
    "        \"medium\": {\"base_channels\": 96, \"latent_dim\": 192, \"epochs\": 40, \"lr\": 2e-4},\n",
    "    },\n",
    "    \"pixelcnn\": {\n",
    "        \"small\": {\"hidden_channels\": 64, \"residual_layers\": 5, \"epochs\": 30, \"lr\": 3e-4},\n",
    "        \"medium\": {\"hidden_channels\": 96, \"residual_layers\": 7, \"epochs\": 40, \"lr\": 3e-4},\n",
    "    },\n",
    "    \"ddpm\": {\n",
    "        \"small\": {\"base_channels\": 64, \"time_channels\": 256, \"timesteps\": 1000, \"epochs\": 30, \"lr\": 2e-4},\n",
    "        \"medium\": {\"base_channels\": 96, \"time_channels\": 256, \"timesteps\": 750, \"epochs\": 40, \"lr\": 2e-4},\n",
    "    },\n",
    "}\n",
    "\n",
    "EXPERIMENT_CONFIGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dcgan': 1024, 'vae': 1024, 'pixelcnn': 1024, 'ddpm': 1024}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THROUGHPUT_SAMPLES = {\n",
    "    \"dcgan\": 1024,\n",
    "    \"vae\": 1024,\n",
    "    \"pixelcnn\": 1024,  # autoregressive sampling is slow; document your actual sample count if you change this\n",
    "    \"ddpm\": 1024,\n",
    "}\n",
    "\n",
    "THROUGHPUT_SAMPLES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c12062",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 391, Validation batches: 79\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=True,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=False,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "train_loader, val_loader = get_dataloaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 391, Validation batches: 79\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=True,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=False,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "train_loader, val_loader = get_dataloaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60008d8",
   "metadata": {},
   "source": [
    "## Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "    \n",
    "def to_device(batch, device=DEVICE):\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def images_to_uint8(tensor: torch.Tensor) -> np.ndarray:\n",
    "    tensor = tensor.detach().cpu().clamp(0.0, 1.0)\n",
    "    tensor = (tensor * 255.0).round().to(torch.uint8)\n",
    "    return tensor.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "\n",
    "def save_image_grid(images: torch.Tensor, path: Path, nrow: int = 8):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    grid = make_grid(images.detach().cpu(), nrow=nrow, padding=2)\n",
    "    save_image(grid, str(path))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def gather_real_images(loader: DataLoader, max_samples: int = 2048) -> torch.Tensor:\n",
    "    batches = []\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        imgs = batch[\"images\"]\n",
    "        batches.append(imgs)\n",
    "        total += imgs.size(0)\n",
    "        if total >= max_samples:\n",
    "            break\n",
    "    return torch.cat(batches, dim=0)[:max_samples]\n",
    "\n",
    "class _ArrayDataset(Dataset):\n",
    "    def __init__(self, array: np.ndarray):\n",
    "        self.array = array\n",
    "    def __len__(self) -> int:\n",
    "        return self.array.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        arr = self.array[idx]\n",
    "        tensor = torch.from_numpy(arr).permute(2, 0, 1).contiguous()\n",
    "        return tensor\n",
    "        \n",
    "def compute_kid_score(real_images: torch.Tensor, fake_images: torch.Tensor) -> float:\n",
    "    real_np = images_to_uint8(real_images)\n",
    "    fake_np = images_to_uint8(fake_images)\n",
    "\n",
    "\n",
    "    metrics = calculate_metrics(\n",
    "        input1=_ArrayDataset(fake_np),\n",
    "        input2=_ArrayDataset(real_np),\n",
    "        kid=True,\n",
    "        fid=False,\n",
    "        verbose=True,\n",
    "        ProgressBar=True,\n",
    "        cuda=True,\n",
    "        show_progress_bar=True,\n",
    "        save_cpu_ram=True\n",
    "    )\n",
    "    kid_key = \"kernel_inception_distance_mean\"\n",
    "    if kid_key not in metrics:\n",
    "        kid_key = \"kid_mean\"\n",
    "    kid_value = metrics.get(kid_key)\n",
    "    if kid_value is None:\n",
    "        raise KeyError(f\"KID metric missing expected keys: {list(metrics.keys())}\")\n",
    "    return float(kid_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def measure_sampling_throughput(sample_fn, num_images: int = 1024, device=DEVICE):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    start = time.perf_counter()\n",
    "    samples = sample_fn(num_images=num_images, device=device)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    elapsed = time.perf_counter() - start\n",
    "    throughput = num_images / elapsed\n",
    "    return samples, elapsed, throughput\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "289c6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real subset cached: torch.Size([2048, 3, 32, 32]) (device copy: torch.Size([2048, 3, 32, 32]))\n"
     ]
    }
   ],
   "source": [
    "real_subset = gather_real_images(val_loader, max_samples=2048)\n",
    "real_subset_device = real_subset.to(DEVICE)\n",
    "print(f\"Real subset cached: {real_subset.shape} (device copy: {real_subset_device.shape})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246b7c3",
   "metadata": {},
   "source": [
    "## Training loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_history():\n",
    "    return defaultdict(list)\n",
    "  \n",
    "def train_dcgan(model: DCGAN, dataloader: DataLoader, optimizer_g, optimizer_d, epochs: int = 5, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[DCGAN] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            optimizer_d.zero_grad(set_to_none=True)\n",
    "            out_d = model({\"images\": images})\n",
    "            loss_d = out_d[\"discriminator_loss\"]\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            optimizer_g.zero_grad(set_to_none=True)\n",
    "            out_g = model({\"images\": images})\n",
    "            loss_g = out_g[\"generator_loss\"]\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            history[\"d_loss\"].append(loss_d.item())\n",
    "            history[\"g_loss\"].append(loss_g.item())\n",
    "            progress.set_postfix({\"d\": loss_d.item(), \"g\": loss_g.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_vae(model: ConvVAE, dataloader: DataLoader, optimizer, epochs: int = 5, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[VAE] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history[\"loss\"].append(loss.item())\n",
    "            history[\"kl\"].append(out[\"kl\"].mean().item())\n",
    "            history[\"recon\"].append(out[\"reconstruction_loss\"].mean().item())\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_pixelcnn(model: PixelCNN, dataloader: DataLoader, optimizer, epochs: int = 5, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[PixelCNN] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history[\"loss\"].append(loss.item())\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_ddpm(model: DenoiseUNet, dataloader: DataLoader, optimizer, epochs: int = 1, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[DDPM] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history[\"loss\"].append(loss.item())\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed83abe",
   "metadata": {},
   "source": [
    "## Sampling & evaluation helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_dcgan(model: DCGAN, num_samples: int = 64, device=DEVICE, batch_size: int = 64):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    samples = []\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        n = min(batch_size, num_samples - start)\n",
    "        z = torch.randn(n, model.latent_dim, device=device)\n",
    "        fake = model.sample(z)### TODO: sample from the model\n",
    "        samples.append(fake.detach().cpu())\n",
    "    return torch.cat(samples, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_vae(model: ConvVAE, num_samples: int = 64, device=DEVICE, batch_size: int = 64):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    samples = []\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        n = min(batch_size, num_samples - start)\n",
    "        z = torch.randn(n, model.latent_dim, device=device)\n",
    "        mean, logvar = model.decode(z) ### TODO: decode the latent codes using the model\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        draw = mean + std * torch.randn_like(std) ### TODO: sample from the distribution\n",
    "        samples.append(draw.detach().cpu())\n",
    "    return torch.cat(samples, dim=0).clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_pixelcnn(model: PixelCNN, num_samples: int = 16, device=DEVICE, image_size: int = IMAGE_SIZE):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    bins = model.bins\n",
    "    samples = torch.zeros(num_samples, model.image_channels, image_size, image_size, device=device)\n",
    "    for row in range(image_size):\n",
    "        for col in range(image_size):\n",
    "            logits = model({\"images\": samples})[\"logits\"] ### TODO get the logits from the model\n",
    "            logits = logits.view(num_samples, model.image_channels, bins, image_size, image_size)\n",
    "            pixel_logits = logits[:, :, :, row, col]\n",
    "            probs = torch.softmax(pixel_logits, dim=2) ### TODO: Softmax the logits to get the probability of each bin\n",
    "            cat = torch.distributions.Categorical(probs=probs)\n",
    "            pixel = cat.sample()\n",
    "            pixel = pixel.float() / (bins - 1)\n",
    "            samples[:, :, row, col] = pixel\n",
    "    return samples.detach().cpu().clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _ddpm_predict_noise(model: DenoiseUNet, xt: torch.Tensor, t: torch.Tensor):\n",
    "    time_emb = model.time_embedding(t)\n",
    "    h0 = model.model[\"init\"](xt)#input_blocks[0](xt, time_emb) ### TODO: forward the input through the layers\n",
    "    skip0, h1 = model.model[\"down0\"](h0, time_emb) ### TODO\n",
    "    skip1, h2 = model.model[\"down1\"](h1, time_emb) ### TODO\n",
    "    skip2, h3 = model.model[\"down2\"](h2, time_emb) ### TODO\n",
    "    h_mid = model.model[\"mid\"](h3, time_emb) ### TODO\n",
    "    h = model.model[\"up2\"](h_mid, skip2, time_emb) ### TODO\n",
    "    h = model.model[\"up1\"](h, skip1, time_emb) ### TODO\n",
    "    h = model.model[\"up0\"](h, skip0, time_emb) ### TODO\n",
    "    pred_noise = model.model[\"out\"](h) ### TODO\n",
    "    return pred_noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(model: DenoiseUNet, num_samples: int = 64, device=DEVICE, timesteps: int = None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    T = timesteps if timesteps is not None else model.timesteps\n",
    "    betas = model.betas.to(device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = model.alphas_cumprod.to(device)\n",
    "    alphas_cumprod_prev = torch.cat([torch.ones(1, device=device), alphas_cumprod[:-1]])\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "    sqrt_one_minus_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "    xt = torch.randn(num_samples, model.image_channels, IMAGE_SIZE, IMAGE_SIZE, device=device)\n",
    "    for step in reversed(range(T)):\n",
    "        t = torch.full((num_samples,), step, device=device, dtype=torch.long)\n",
    "        pred_noise = _ddpm_predict_noise(model, xt, t) ### TODO: predict the noise from the model\n",
    "        beta_t = betas[step]\n",
    "        sqrt_recip_alpha_t = sqrt_recip_alphas[step]\n",
    "        sqrt_one_minus_cumprod_t = sqrt_one_minus_cumprod[step]\n",
    "        model_mean = sqrt_recip_alpha_t * (xt - beta_t / sqrt_one_minus_cumprod_t * pred_noise)\n",
    "        if step > 0:\n",
    "            variance = beta_t * (1.0 - alphas_cumprod_prev[step]) / (1.0 - alphas_cumprod[step])\n",
    "            noise = torch.randn_like(xt)\n",
    "            xt = model_mean + torch.sqrt(variance) * noise ### TODO: update the image\n",
    "        else:\n",
    "            xt = model_mean\n",
    "    return xt.detach().cpu().clamp(-1.0, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21c0d6",
   "metadata": {},
   "source": [
    "## Experiment tracking utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_log = {}\n",
    "\n",
    "\n",
    "def record_metrics(model_name: str, variant: str, metrics: dict):\n",
    "    experiment_log.setdefault(model_name, {})[variant] = metrics\n",
    "\n",
    "\n",
    "def show_metrics(model_name: str = None):\n",
    "    if model_name is None:\n",
    "        for name in experiment_log:\n",
    "            show_metrics(name)\n",
    "        return\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    entries = experiment_log.get(model_name, {})\n",
    "    for variant, metrics in entries.items():\n",
    "        print(f\"  [{variant}]\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "    if not entries:\n",
    "        print(\"  (no entries yet)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9bfb2",
   "metadata": {},
   "source": [
    "## DCGAN (adversarial generation)\n",
    "\n",
    "Fill in the TODOs inside `models/gan.py` before running this section. The cells below:\n",
    "\n",
    "1. Instantiate the model and optimizers.\n",
    "2. Train for the baseline configuration.\n",
    "3. Generate sample grids, measure KID, and record sampling throughput.\n",
    "4. Repeat with the medium-scale variant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline DCGAN parameters: 1,846,980\n"
     ]
    }
   ],
   "source": [
    "dcgan_cfg = EXPERIMENT_CONFIGS[\"dcgan\"][\"small\"]\n",
    "dcgan = DCGAN(\n",
    "    image_channels=3,\n",
    "    latent_dim=dcgan_cfg[\"latent_dim\"],\n",
    "    base_channels=dcgan_cfg[\"base_channels\"],\n",
    ")\n",
    "optim_g = torch.optim.Adam(dcgan.generator.parameters(), lr=dcgan_cfg[\"lr\"], betas=(0.5, 0.999))\n",
    "optim_d = torch.optim.Adam(dcgan.discriminator.parameters(), lr=dcgan_cfg[\"lr\"], betas=(0.5, 0.999))\n",
    "print(f\"Baseline DCGAN parameters: {count_parameters(dcgan):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished DCGAN training: 11730 steps\n",
      "DCGAN training time per epoch (s): [7.19, 7.1, 7.04, 7.01, 7.02, 6.97, 6.99, 6.98, 7.08, 6.99, 7.05, 7.08, 6.98, 7.02, 7.06, 6.97, 6.99, 7.0, 7.0, 7.12, 7.07, 6.96, 7.03, 7.0, 6.98, 7.02, 6.98, 7.05, 7.05, 7.11]\n"
     ]
    }
   ],
   "source": [
    "DCGAN_EPOCHS = dcgan_cfg[\"epochs\"]\n",
    "\n",
    "dcgan_history = train_dcgan(dcgan, train_loader, optimizer_g=optim_g, optimizer_d=optim_d, epochs=DCGAN_EPOCHS)\n",
    "dcgan_epoch_times = dcgan_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "dcgan_train_time = sum(dcgan_epoch_times)\n",
    "print(f\"Finished DCGAN training: {len(dcgan_history['d_loss'])} steps\")\n",
    "print(f\"DCGAN training time per epoch (s): {[round(t, 2) for t in dcgan_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting features from input1\n",
      "Processing samples                                                          \n",
      "Extracting features from input2\n",
      "Processing samples                                                           \n",
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KID (x10^3): 0.059\n",
      "Sampling time for 1024 images: 0.02 s (throughput 50016.6 img/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Inception Distance: 0.05922116756439209 ± 0.00024084855621544541\n"
     ]
    }
   ],
   "source": [
    "# TODO: run once `sample_dcgan` is implemented.\n",
    "# Evaluation: sampling, KID, throughput\n",
    "samples_64 = sample_dcgan(dcgan, num_samples=64, device=DEVICE)\n",
    "samples_64_vis = (samples_64 + 1.0) / 2.0  # map from [-1, 1] to [0, 1]\n",
    "save_image_grid(samples_64_vis, ARTIFACT_DIR / \"dcgan_samples_baseline.png\", nrow=8)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"dcgan\"]\n",
    "samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: (sample_dcgan(dcgan, num_samples=num_images, device=device) + 1.0) / 2.0,\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:samples_large.size(0)].cpu(), samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"DCGAN\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": DCGAN_EPOCHS,\n",
    "        \"params\": count_parameters(dcgan),\n",
    "        \"kid\": kid,\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": dcgan_train_time,\n",
    "        \"train_time_per_epoch_s\": dcgan_train_time / DCGAN_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.1f} img/s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d1a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb4f782f",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"DCGAN\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9204c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43062489",
   "metadata": {},
   "source": [
    "## Convolutional VAE (latent variable model)\n",
    "\n",
    "Make sure the loss components in `models/vae.py` are implemented before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline VAE parameters: 3,518,214\n"
     ]
    }
   ],
   "source": [
    "vae_cfg = EXPERIMENT_CONFIGS[\"vae\"][\"small\"]\n",
    "vae = ConvVAE(\n",
    "    image_channels=3,\n",
    "    latent_dim=vae_cfg[\"latent_dim\"],\n",
    "    base_channels=vae_cfg[\"base_channels\"],\n",
    ")\n",
    "optim_vae = torch.optim.Adam(vae.parameters(), lr=vae_cfg[\"lr\"])\n",
    "print(f\"Baseline VAE parameters: {count_parameters(vae):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished VAE training: 11730 steps\n",
      "VAE training time per epoch (s): [5.03, 5.27, 5.18, 5.48, 5.42, 5.52, 5.46, 5.57, 5.44, 5.3, 5.31, 5.5, 5.57, 5.5, 5.66, 6.27, 5.63, 6.04, 5.4, 5.65, 6.33, 6.62, 5.53, 5.64, 5.61, 5.61, 5.55, 5.31, 5.22, 5.21]\n"
     ]
    }
   ],
   "source": [
    "VAE_EPOCHS = vae_cfg[\"epochs\"]\n",
    "\n",
    "vae_history = train_vae(vae, train_loader, optimizer=optim_vae, epochs=VAE_EPOCHS)\n",
    "vae_epoch_times = vae_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "vae_train_time = sum(vae_epoch_times)\n",
    "print(f\"Finished VAE training: {len(vae_history['loss'])} steps\")\n",
    "print(f\"VAE training time per epoch (s): {[round(t, 2) for t in vae_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting features from input1\n",
      "Processing samples                                                           \n",
      "Extracting features from input2\n",
      "Processing samples                                                           \n",
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ELBO: -3623.2022 (recon -3861.3095, KL 238.1072)\n",
      "KID (x10^3): 0.296\n",
      "Sampling time for 1024 images: 0.02 s (throughput 49088.7 img/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Inception Distance: 0.296482617855072 ± 0.0005606269169009054\n"
     ]
    }
   ],
   "source": [
    "def evaluate_vae_elbo(model: ConvVAE, loader: DataLoader, device=DEVICE):\n",
    "    model.eval()\n",
    "    total_loss = total_recon = total_kl = 0.0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            out = model({\"images\": images})\n",
    "            batch_size = images.size(0)\n",
    "            total_examples += batch_size\n",
    "            total_loss += out[\"loss\"].item() * batch_size\n",
    "            total_recon += out[\"reconstruction_loss\"].mean().item() * batch_size\n",
    "            total_kl += out[\"kl\"].mean().item() * batch_size\n",
    "    return {\n",
    "        \"loss\": total_loss / total_examples,\n",
    "        \"reconstruction\": total_recon / total_examples,\n",
    "        \"kl\": total_kl / total_examples,\n",
    "    }\n",
    "\n",
    "\n",
    "# TODO: once `sample_vae' are implemented, run the evaluation below.\n",
    "\n",
    "vae_elbo = evaluate_vae_elbo(vae, val_loader)\n",
    "vae_samples = sample_vae(vae, num_samples=64, device=DEVICE)\n",
    "save_image_grid(vae_samples, ARTIFACT_DIR / \"vae_samples_baseline.png\", nrow=8)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"vae\"]\n",
    "vae_samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: sample_vae(vae, num_samples=num_images, device=device),\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:vae_samples_large.size(0)].cpu(), vae_samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"VAE\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": VAE_EPOCHS,\n",
    "        \"params\": count_parameters(vae),\n",
    "        \"kid\": kid,\n",
    "        \"nll\": vae_elbo[\"loss\"],\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": vae_train_time,\n",
    "        \"train_time_per_epoch_s\": vae_train_time / VAE_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Validation ELBO: {vae_elbo['loss']:.4f} (recon {vae_elbo['reconstruction']:.4f}, KL {vae_elbo['kl']:.4f})\")\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {vae_samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.1f} img/s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f063c7",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** Retrain the medium-scale configuration and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"VAE\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6e80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7670317f",
   "metadata": {},
   "source": [
    "## PixelCNN (autoregressive generation)\n",
    "\n",
    "Ensure the masking logic and loss in `models/pixelcnn.py` are implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline PixelCNN parameters: 268,992\n"
     ]
    }
   ],
   "source": [
    "pixelcnn_cfg = EXPERIMENT_CONFIGS[\"pixelcnn\"][\"small\"]\n",
    "pixelcnn = PixelCNN(\n",
    "    image_channels=3,\n",
    "    hidden_channels=pixelcnn_cfg[\"hidden_channels\"],\n",
    "    residual_layers=pixelcnn_cfg[\"residual_layers\"],\n",
    "    bins=256,\n",
    ")\n",
    "optim_pixelcnn = torch.optim.Adam(pixelcnn.parameters(), lr=pixelcnn_cfg[\"lr\"])\n",
    "print(f\"Baseline PixelCNN parameters: {count_parameters(pixelcnn):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished PixelCNN training: 11730 steps\n",
      "PixelCNN training time per epoch (s): [17.8, 17.83, 18.19, 18.22, 17.94, 18.02, 17.81, 17.93, 18.23, 18.32, 18.18, 18.24, 17.98, 18.86, 18.26, 18.14, 18.05, 18.48, 18.27, 18.07, 17.95, 18.13, 18.74, 18.09, 17.78, 17.9, 17.79, 17.78, 17.7, 17.83]\n"
     ]
    }
   ],
   "source": [
    "PIXELCNN_EPOCHS = pixelcnn_cfg[\"epochs\"]\n",
    "\n",
    "pixelcnn_history = train_pixelcnn(pixelcnn, train_loader, optimizer=optim_pixelcnn, epochs=PIXELCNN_EPOCHS)\n",
    "pixelcnn_epoch_times = pixelcnn_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "pixelcnn_train_time = sum(pixelcnn_epoch_times)\n",
    "print(f\"Finished PixelCNN training: {len(pixelcnn_history['loss'])} steps\")\n",
    "print(f\"PixelCNN training time per epoch (s): {[round(t, 2) for t in pixelcnn_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting features from input1\n",
      "Processing samples                                                           \n",
      "Extracting features from input2\n",
      "Processing samples                                                           \n",
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation NLL: 3.8219\n",
      "KID (x10^3): 0.161\n",
      "Sampling time for 1024 images: 283.82 s (throughput 3.61 img/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Inception Distance: 0.161173038482666 ± 0.0004256044615210478\n"
     ]
    }
   ],
   "source": [
    "def evaluate_pixelcnn_nll(model: PixelCNN, loader: DataLoader, device=DEVICE):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            batch_size = images.size(0)\n",
    "            total_examples += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "# TODO: run once `sample_pixelcnn` is ready.\n",
    "pixelcnn_nll = evaluate_pixelcnn_nll(pixelcnn, val_loader)\n",
    "pixelcnn_samples = sample_pixelcnn(pixelcnn, num_samples=16, device=DEVICE)\n",
    "save_image_grid(pixelcnn_samples, ARTIFACT_DIR / \"pixelcnn_samples_baseline.png\", nrow=4)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"pixelcnn\"]\n",
    "pixelcnn_samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: sample_pixelcnn(pixelcnn, num_samples=num_images, device=device),\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:pixelcnn_samples_large.size(0)].cpu(), pixelcnn_samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"PixelCNN\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": PIXELCNN_EPOCHS,\n",
    "        \"params\": count_parameters(pixelcnn),\n",
    "        \"kid\": kid,\n",
    "        \"nll\": pixelcnn_nll,\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": pixelcnn_train_time,\n",
    "        \"train_time_per_epoch_s\": pixelcnn_train_time / PIXELCNN_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Validation NLL: {pixelcnn_nll:.4f}\")\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {pixelcnn_samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.2f} img/s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e21e0a",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** > **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"PixelCNN\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99a441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "733efc6d",
   "metadata": {},
   "source": [
    "## DDPM (diffusion model)\n",
    "\n",
    "Verify the diffusion loss path in `models/ddpm.py` before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline DDPM parameters: 6,993,987\n"
     ]
    }
   ],
   "source": [
    "ddpm_cfg = EXPERIMENT_CONFIGS[\"ddpm\"][\"small\"]\n",
    "ddpm = DenoiseUNet(\n",
    "    image_channels=3,\n",
    "    base_channels=ddpm_cfg[\"base_channels\"],\n",
    "    time_channels=ddpm_cfg[\"time_channels\"],\n",
    "    timesteps=ddpm_cfg[\"timesteps\"],\n",
    ")\n",
    "optim_ddpm = torch.optim.Adam(ddpm.parameters(), lr=ddpm_cfg[\"lr\"])\n",
    "print(f\"Baseline DDPM parameters: {count_parameters(ddpm):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished DDPM training: 11730 steps\n",
      "DDPM training time per epoch (s): [26.01, 25.8, 25.78, 25.84, 25.96, 25.81, 25.92, 25.86, 25.96, 25.82, 25.8, 25.97, 25.78, 25.8, 25.99, 27.02, 44.45, 38.96, 36.77, 36.78, 38.73, 32.58, 32.46, 32.08, 32.97, 33.04, 33.54, 33.97, 35.79, 39.02]\n"
     ]
    }
   ],
   "source": [
    "DDPM_EPOCHS = ddpm_cfg[\"epochs\"]\n",
    "\n",
    "ddpm_history = train_ddpm(ddpm, train_loader, optimizer=optim_ddpm, epochs=DDPM_EPOCHS)\n",
    "ddpm_epoch_times = ddpm_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "ddpm_train_time = sum(ddpm_epoch_times)\n",
    "print(f\"Finished DDPM training: {len(ddpm_history['loss'])} steps\")\n",
    "print(f\"DDPM training time per epoch (s): {[round(t, 2) for t in ddpm_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Extracting features from input1\n",
      "Processing samples                                                           \n",
      "Extracting features from input2\n",
      "Processing samples                                                           \n",
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KID (x10^3): 0.060\n",
      "Sampling time for 1024 images: 181.99 s (throughput 5.63 img/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Inception Distance: 0.05981858730316162 ± 0.0002560027266702629\n"
     ]
    }
   ],
   "source": [
    "# TODO: once `sample_ddpm` is implemented, run the evaluation block below.\n",
    "ddpm_samples = sample_ddpm(ddpm, num_samples=64, device=DEVICE)\n",
    "ddpm_samples_vis = (ddpm_samples + 1.0) / 2.0\n",
    "save_image_grid(ddpm_samples_vis, ARTIFACT_DIR / \"ddpm_samples_baseline.png\", nrow=8)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"ddpm\"]\n",
    "samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: (sample_ddpm(ddpm, num_samples=num_images, device=device) + 1.0) / 2.0,\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:samples_large.size(0)].cpu(), samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"DDPM\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": DDPM_EPOCHS,\n",
    "        \"params\": count_parameters(ddpm),\n",
    "        \"kid\": kid,\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": ddpm_train_time,\n",
    "        \"train_time_per_epoch_s\": ddpm_train_time / DDPM_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.2f} img/s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7a276",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"DDPM\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfba670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a274362b",
   "metadata": {},
   "source": [
    "## Summary & export\n",
    "\n",
    "After running the experiments above (small and medium variants), use the helper below to view logged metrics and export them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DCGAN ===\n",
      "  [baseline]\n",
      "    epochs: 30\n",
      "    params: 1846980\n",
      "    kid: 0.05922116756439209\n",
      "    sampling_time_s: 0.020473199998377822\n",
      "    throughput_img_per_s: 50016.607080531416\n",
      "    train_time_total_s: 210.89742730004946\n",
      "    train_time_per_epoch_s: 7.029914243334982\n",
      "=== VAE ===\n",
      "  [baseline]\n",
      "    epochs: 30\n",
      "    params: 3518214\n",
      "    kid: 0.296482617855072\n",
      "    nll: -3623.202247265625\n",
      "    sampling_time_s: 0.020860199991147965\n",
      "    throughput_img_per_s: 49088.695239476845\n",
      "    train_time_total_s: 166.83106550009688\n",
      "    train_time_per_epoch_s: 5.561035516669896\n",
      "=== PixelCNN ===\n",
      "  [baseline]\n",
      "    epochs: 30\n",
      "    params: 268992\n",
      "    kid: 0.161173038482666\n",
      "    nll: 3.8218883686065674\n",
      "    sampling_time_s: 283.8230419999891\n",
      "    throughput_img_per_s: 3.6078818435045856\n",
      "    train_time_total_s: 542.5294239999348\n",
      "    train_time_per_epoch_s: 18.08431413333116\n",
      "=== DDPM ===\n",
      "  [baseline]\n",
      "    epochs: 30\n",
      "    params: 6993987\n",
      "    kid: 0.05981858730316162\n",
      "    sampling_time_s: 181.98717189999297\n",
      "    throughput_img_per_s: 5.626770224017309\n",
      "    train_time_total_s: 916.2292487999948\n",
      "    train_time_per_epoch_s: 30.540974959999826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>params</th>\n",
       "      <th>kid</th>\n",
       "      <th>sampling_time_s</th>\n",
       "      <th>throughput_img_per_s</th>\n",
       "      <th>train_time_total_s</th>\n",
       "      <th>train_time_per_epoch_s</th>\n",
       "      <th>nll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>variant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DCGAN</th>\n",
       "      <th>baseline</th>\n",
       "      <td>30</td>\n",
       "      <td>1846980</td>\n",
       "      <td>0.059221</td>\n",
       "      <td>0.020473</td>\n",
       "      <td>50016.607081</td>\n",
       "      <td>210.897427</td>\n",
       "      <td>7.029914</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAE</th>\n",
       "      <th>baseline</th>\n",
       "      <td>30</td>\n",
       "      <td>3518214</td>\n",
       "      <td>0.296483</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>49088.695239</td>\n",
       "      <td>166.831066</td>\n",
       "      <td>5.561036</td>\n",
       "      <td>-3623.202247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PixelCNN</th>\n",
       "      <th>baseline</th>\n",
       "      <td>30</td>\n",
       "      <td>268992</td>\n",
       "      <td>0.161173</td>\n",
       "      <td>283.823042</td>\n",
       "      <td>3.607882</td>\n",
       "      <td>542.529424</td>\n",
       "      <td>18.084314</td>\n",
       "      <td>3.821888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DDPM</th>\n",
       "      <th>baseline</th>\n",
       "      <td>30</td>\n",
       "      <td>6993987</td>\n",
       "      <td>0.059819</td>\n",
       "      <td>181.987172</td>\n",
       "      <td>5.626770</td>\n",
       "      <td>916.229249</td>\n",
       "      <td>30.540975</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   epochs   params       kid  sampling_time_s  \\\n",
       "model    variant                                                \n",
       "DCGAN    baseline      30  1846980  0.059221         0.020473   \n",
       "VAE      baseline      30  3518214  0.296483         0.020860   \n",
       "PixelCNN baseline      30   268992  0.161173       283.823042   \n",
       "DDPM     baseline      30  6993987  0.059819       181.987172   \n",
       "\n",
       "                   throughput_img_per_s  train_time_total_s  \\\n",
       "model    variant                                              \n",
       "DCGAN    baseline          50016.607081          210.897427   \n",
       "VAE      baseline          49088.695239          166.831066   \n",
       "PixelCNN baseline              3.607882          542.529424   \n",
       "DDPM     baseline              5.626770          916.229249   \n",
       "\n",
       "                   train_time_per_epoch_s          nll  \n",
       "model    variant                                        \n",
       "DCGAN    baseline                7.029914          NaN  \n",
       "VAE      baseline                5.561036 -3623.202247  \n",
       "PixelCNN baseline               18.084314     3.821888  \n",
       "DDPM     baseline               30.540975          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics summary to c:\\Users\\sidne\\OneDrive\\Bureau\\Automne2025\\school\\Representationh_L-IFT6135\\homeworks\\hw3\\part1\\artifacts\\metrics_summary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "show_metrics()\n",
    "\n",
    "rows = []\n",
    "for model_name, variants in experiment_log.items():\n",
    "    for variant, metrics in variants.items():\n",
    "        row = {\"model\": model_name, \"variant\": variant}\n",
    "        row.update(metrics)\n",
    "        rows.append(row)\n",
    "if rows:\n",
    "    df_metrics = pd.DataFrame(rows)\n",
    "    display(df_metrics.set_index([\"model\", \"variant\"]))\n",
    "\n",
    "summary_path = ARTIFACT_DIR / \"metrics_summary.json\"\n",
    "with summary_path.open(\"w\") as fp:\n",
    "    json.dump(experiment_log, fp, indent=2)\n",
    "print(f\"Saved metrics summary to {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c40f0f",
   "metadata": {},
   "source": [
    "## Aggregate plots\n",
    "\n",
    "Use this section to generate the plots and figures requested in the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3787a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IFT6135",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
