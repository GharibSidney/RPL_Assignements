{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyJ4Y-50zfCt"
      },
      "source": [
        "# Notes: You will have to:\n",
        "  >1- Make a Copy of this notebook to edit it for your solutions;\n",
        "  >\n",
        "  >2- Upload on Gradescope: The `Colab Notebook edited with your solutions`, and a `pdf Report (Discussion questions)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNWoiYrUTh5q"
      },
      "source": [
        "# Question 1: MLP Implementation with Numpy\n",
        "\n",
        "In this exercise, we will explore the construction of a multi-layer perceptron (MLP) by coding it from the ground up. We will develop a deep neural network step-by-step, beginning with a single neuron, progressing to a layer, and ultimately building the entire network. We will then train this network and evaulate on an interesting dataset and try to understand the workings in more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJdB1Yv4T25Z"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ocuxjx05TKdn"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3idgvI3_T-dC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgDzUKUNUK_n"
      },
      "source": [
        "## Q 1.1 (a): Activation Functions\n",
        "\n",
        "In this question you will implement the softmax and ReLU activation functions and their derivatives. Both these functions help introduce non-linearity which gives neural network their capacity. We will implement these functions in the `ActivationFunction` class as static methods. You can access the method later using the following code:\n",
        "\n",
        "```\n",
        "# For softmax activation\n",
        "ActivationFunction.softmax(x)\n",
        "\n",
        "# For calucating the derivative of softmax function\n",
        "ActivationFunction.softmax_derivative(x)\n",
        "\n",
        "# For ReLU activation\n",
        "ActivationFunction.relu(x)\n",
        "\n",
        "# For calculating the derivative of ReLU function\n",
        "ActivationFunction.relu_derivative(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5ZLkQNPWULZe"
      },
      "outputs": [],
      "source": [
        "class ActivationFunction:\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        \"\"\"\n",
        "        Compute the Softmax activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Softmax activation values\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        expo = np.exp(x)\n",
        "        return expo / np.sum(expo)\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the Softmax function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Derivative of Softmax function\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "\n",
        "        exps = np.exp(x - np.max(x))  \n",
        "        s = exps / np.sum(exps)\n",
        "\n",
        "        return  np.diag(s) - np.outer(s, s)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        \"\"\"\n",
        "        Compute the ReLU activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: ReLU activation values\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(x):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the ReLU activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Derivative of ReLU function\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77bPGi4UVCH"
      },
      "source": [
        "## Q 1.1 (b): Neuron Implementation\n",
        "\n",
        "Neuron is the fundamental building block of MLP. A neuron simulates a biological neuron which can be activated or fired when responding to a stimulus. The equation of a neuron is as follow:\n",
        "\n",
        "$$ f(x) = a(\\mathbf{w} \\cdot \\mathbf{x} + b) $$\n",
        "\n",
        "where inputs $\\mathbf{x}$ and weights $\\mathbf{w}$ are multidimensional vectors, bias $b$ is a scalar, $a(.)$ is an activation function.\n",
        "\n",
        "**Note:** For the whole exercise you will be receiving inputs of batch size of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JaME5ZMeUYBz"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, input_size, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize a neuron with random weights and bias.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "            activation_function (function): Activation function to use\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        self.activation_function = activation_function\n",
        "        self.weights = np.random.randn(input_size)\n",
        "        self.bias = np.random.randn()\n",
        "\n",
        "    def activate(self, x):\n",
        "        \"\"\"\n",
        "        Compute the activation of the neuron.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            float: Activation value\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return self.activation_function(self.weights @ x + self.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7nfeRx3UdPQ"
      },
      "source": [
        "## Q 1.1(c): Layer Implementation\n",
        "\n",
        "Now we will compose a layer which consists of multiple neurons. Complete initializing the layer and compute the forward pass by sending the input through each neuron of the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bzrfOWC1UZ-m"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize a layer of neurons.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "            output_size (int): Number of neurons in the layer\n",
        "            activation_function (function): Activation function to use\n",
        "        \"\"\"\n",
        "        ## Write your code here ##\n",
        "        self.neurons = [Neuron(input_size, activation_function) for _ in range(output_size)]\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the forward pass through the layer.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.array: Output of the layer\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return np.array([neuron.activate(x) for neuron in self.neurons])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeUxOv7kUh5z"
      },
      "source": [
        "## Q 1.1(d) + 2 (a) to (d): Neural Network Construction\n",
        "\n",
        "Now we have built all the building blocks required to build our neural network. Complete the implementations using the hints given in the code. There are additional functions that will be useful later. Do not modify these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeVTOZ_uUjrF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, hidden_dim, activation_function_hidden, activation_output):\n",
        "        \"\"\"\n",
        "        Initialize a neural network with specified layer sizes.\n",
        "\n",
        "        Args:\n",
        "            layer_sizes (list): List of integers representing the size of each layer\n",
        "            hiden_dim (int): Dimension of hidden layers\n",
        "            activation_function_hidden (function): Activation function to use for hidden layers\n",
        "            activation_output (function): Activation function to use for output layer\n",
        "        \"\"\"\n",
        "        self.layers = []\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.activation_function_hidden = activation_function_hidden\n",
        "        self.activation_output = activation_output\n",
        "        ## Write your code here ##\n",
        "        #TODO Define the network architecture in the constructor with exactly 2 layers: one hidden layer (64 neurons)\n",
        "        # and one output layer (10 units). (1 point)\n",
        "        # input_size = layer_sizes[0]\n",
        "        # self.layers.append(Layer(input_size, 64, activation_function_hidden))\n",
        "        # self.layers.append(Layer(64, 10, activation_output))\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            input_size = layer_sizes[i]\n",
        "            output_size = layer_sizes[i + 1]\n",
        "            \n",
        "            # Use hidden activation for all layers except the last one\n",
        "            if i < len(layer_sizes) - 2:\n",
        "                activation = activation_function_hidden\n",
        "            else:\n",
        "                activation = activation_output\n",
        "            \n",
        "            self.layers.append(Layer(input_size, output_size, activation))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the forward pass through the entire network.\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "        Returns:\n",
        "            np.array: Output of the network\n",
        "        \"\"\"\n",
        "        ## Write your code here ##\n",
        "        # Implement forward(X) using only NumPy. Apply softmax to the final logits\n",
        "        \n",
        "        self.activations = [x] \n",
        "        self.pre_activations = []\n",
        "        \n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "\n",
        "            weights = np.array([neuron.weights for neuron in layer.neurons])\n",
        "            biases = np.array([neuron.bias for neuron in layer.neurons])\n",
        "            z = weights @ output + biases\n",
        "            self.pre_activations.append(z)\n",
        "            \n",
        "            output = layer.forward(output)\n",
        "            self.activations.append(output)\n",
        "        \n",
        "\n",
        "        exp_logits = np.exp(output - np.max(output))\n",
        "        softmax_output = exp_logits / np.sum(exp_logits)\n",
        "        \n",
        "        self.activations[-1] = softmax_output\n",
        "        \n",
        "        return softmax_output\n",
        "        \n",
        "    def layer_backward(self, delta, layer_index, activations, zs):\n",
        "        \"\"\"\n",
        "        Perform backpropagation for a single layer.\n",
        "\n",
        "        Args:\n",
        "            delta (np.array): Gradient of the loss with respect to the output of the current layer\n",
        "            layer_index (int): Index of the current layer\n",
        "            activations (list): List of activations from the forward pass\n",
        "            zs (list): List of z values from the forward pass\n",
        "\n",
        "        Returns:\n",
        "            tuple: (delta for the previous layer, gradients for weights, gradients for biases)\n",
        "        \"\"\"\n",
        "        layer = self.layers[layer_index]\n",
        "        z = zs[layer_index]\n",
        "        a_prev = activations[layer_index]\n",
        "        a = activations[layer_index + 1]\n",
        "\n",
        "        # Compute activation derivative\n",
        "        if layer_index == len(self.layers) - 1:\n",
        "            delta = self.softmax_derivative(a) * delta\n",
        "        else:\n",
        "            if self.activation_function_hidden == ActivationFunction.relu:\n",
        "                delta = self.relu_derivative(z) * delta\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "        gradient_w = np.outer(delta, a_prev)\n",
        "        gradient_b = delta\n",
        "\n",
        "        weights = np.array([neuron.weights for neuron in layer.neurons])\n",
        "\n",
        "        # Compute delta to pass back to previous layer\n",
        "        delta_prev = np.dot(weights.T, delta)\n",
        "\n",
        "        return delta_prev, gradient_w, gradient_b\n",
        "\n",
        "\n",
        "    def backward(self, x, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform backpropagation and update weights.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "            y (np.array): True labels\n",
        "            learning_rate (float): Learning rate for weight updates\n",
        "        \"\"\"\n",
        "        # Forward pass\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for layer in self.layers:\n",
        "            z = np.array([np.dot(neuron.weights, activations[-1]) + neuron.bias for neuron in layer.neurons])\n",
        "            zs.append(z)\n",
        "            activation = layer.forward(activations[-1])\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = activations[-1] - y # difference between prediction and true label \n",
        "        for i in reversed(range(len(self.layers))): # revert because we start from the end and go back to beginning\n",
        "            ## Write your code here ##\n",
        "            delta, gradient_w, gradient_b = self.layer_backward(delta, i, activations, zs)\n",
        "\n",
        "            # Update weights and biases\n",
        "            for j, neuron in enumerate(self.layers[i].neurons):\n",
        "                ## Write your code here ##\n",
        "                neuron.weights -= learning_rate * gradient_w[j]\n",
        "                neuron.bias -= learning_rate * gradient_b[j]\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Training features\n",
        "            y (np.array): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            learning_rate (float): Learning rate for weight updates\n",
        "        \"\"\"\n",
        "        # write your code here\n",
        "        # train_losses = []\n",
        "        # for epoch in range(epochs):\n",
        "        #     epoch_loss = 0\n",
        "        #     Write your codes here\n",
        "        #\n",
        "        #     if epoch % 50 == 0:\n",
        "        #         print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, predictions, targets):\n",
        "        \"Compute loss for model predictions and ground truth\"\n",
        "        # write your code here\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions for a set of input features.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.array: Predictions\n",
        "        \"\"\"\n",
        "        return np.array([self.forward(x) for x in X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JicJ29ryUlDS"
      },
      "source": [
        "# Q 1.2(d): Training and Visualizing Neural Network\n",
        "\n",
        "In the rest of the exercise we will use the components we built earlier to train a neural network and visualise the loss curve. To make things simple we will be building neural networks with only one hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vO7ogsLUnP2"
      },
      "source": [
        "We will use the MNIST dataset for all our experiements which consists of 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gYj9nvIUqyr"
      },
      "outputs": [],
      "source": [
        "# Uncomment the below code to create the dataset. And comment it when submitting to gradescope\n",
        "\n",
        "# np.random.seed(42) # Do not change\n",
        "# For easy data loading and processing, we're using torch here. You're not allowed to use torch for Neural Net components though.\n",
        "def get_train_test_dataset():\n",
        "    import torch\n",
        "    from torchvision import datasets, transforms\n",
        "\n",
        "    # Define a transform to convert images to tensors and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Load the MNIST dataset\n",
        "    mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Take a subset of 1000 samples\n",
        "    X = mnist_dataset.data[:1000].float() / 255.0  # Normalize pixel values to [0,1]\n",
        "    y = mnist_dataset.targets[:1000]\n",
        "\n",
        "    # Creat test / test split\n",
        "\n",
        "    # to be completed\n",
        "    # print(\"Training set shape:\", )\n",
        "    # print(\"Test set shape :\", )\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSq1qSzQFPy0"
      },
      "source": [
        "##Let's visualize a few samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "t1nM7oF6FNJd",
        "outputId": "1eff200a-38b2-4d91-951e-cdb6ba395f3c"
      },
      "outputs": [],
      "source": [
        "# Plot a few images\n",
        "\n",
        "def visualize_mnist_samples(X, y, num_samples=9):\n",
        "    \"\"\"Visualize MNIST sample images\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(min(num_samples, len(X))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        # Reshape from flattened to 28x28 for visualization\n",
        "        img = X[i].reshape(28, 28)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Label: {y[i]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle('MNIST Sample Images')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTVzbcSUugW"
      },
      "source": [
        "Below you have been given the code to train your neural network. and visualize the decision boundry. Modify the hyperparameters to train and visualize the neural network and report your findings in the practical report. We will be testing our model on the training set. The `plot_decision_boundary` method takes the data created earlier and the neural network you trained to show the decision boundry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGqDsjU7Uu6b"
      },
      "outputs": [],
      "source": [
        "def train_network(hidden_layer_size, hidden_dim, epochs=10, activation_hidden=None, activation_output=None, learning_rate=0.001):\n",
        "    input_size, output_size = 28 * 28, 10\n",
        "    layers = [input_size, hidden_layer_size, output_size]\n",
        "\n",
        "    nn = NeuralNetwork(layers, hidden_dim, activation_hidden, activation_output)\n",
        "\n",
        "\n",
        "    # --- Prepare training data ---\n",
        "    X_train_flat =\n",
        "    y_train_np =\n",
        "\n",
        "    # One-hot encode labels\n",
        "    y_train_onehot =\n",
        "\n",
        "    # Train\n",
        "    train_loss = nn.train(X_train_flat, y_train_onehot, epochs=epochs, learning_rate=learning_rate)\n",
        "\n",
        "    # --- Evaluate on training data ---\n",
        "    # preds_train =\n",
        "    # train_accuracy =\n",
        "\n",
        "    # --- Evaluate on test data ---\n",
        "    # X_test_flat =\n",
        "    # y_test_np =\n",
        "    # preds_test =\n",
        "    # test_accuracy =\n",
        "\n",
        "    return nn, train_loss, train_accuracy, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmQowEJgUySb"
      },
      "outputs": [],
      "source": [
        "# Write the code to train and visualize\n",
        "# Consider commenting this out when submitting to gradescope\n",
        "\n",
        "# hidden_layer_size =\n",
        "# hidden_dim =\n",
        "# epochs =\n",
        "# activation_hidden =\n",
        "# activation_output =\n",
        "# learning_rate =\n",
        "# nn, train_loss, train_accuracy, test_accuracy = train_network(hidden_layer_size, hidden_dim  epochs, activation_hidden, activation_output, learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mGVTWsJRpuu"
      },
      "source": [
        "# Q 1.3: Reporting and analysis\n",
        "Analyze your trained model and present findings clearly as asked in the instruction PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adETdDEQR0va"
      },
      "outputs": [],
      "source": [
        "# print(\"Train accuracy: \", train_accuracy)\n",
        "# print(\"Test accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVHN2rRqSEOh"
      },
      "source": [
        "# Training loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RorfYPUfU2sI"
      },
      "outputs": [],
      "source": [
        "# Plot the training loss across 20 epochs\n",
        "\n",
        "def plot_loss_curve(train_loss):\n",
        "  # Write your code here\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7V5IzafMXJF"
      },
      "source": [
        "## Layer activation visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1EaFVfDMb3u"
      },
      "outputs": [],
      "source": [
        "# ===== Latent Space Visualization =====\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def get_penultimate_activations(nn, X_batch):\n",
        "    \"\"\"Get activations from the last hidden layer\"\"\"\n",
        "    # Process each sample individually\n",
        "    activations_list = []\n",
        "    for x in X_batch:\n",
        "        activations = x\n",
        "        for layer in nn.layers[:-1]:  # exclude final output layer\n",
        "            activations = layer.forward(activations)\n",
        "        activations_list.append(activations)\n",
        "    return np.array(activations_list)\n",
        "\n",
        "def visualize_activations(nn, X_test, y_test, digits=(3, 8), method='PCA'):\n",
        "    \"\"\"Visualize layer activations using PCA or t-SNE\"\"\"\n",
        "    # Get activations from penultimate layer\n",
        "    Z = get_penultimate_activations(nn, X_test)\n",
        "\n",
        "    # Apply dimensionality reduction\n",
        "    if method == 'PCA':\n",
        "        reducer = PCA(n_components=2)\n",
        "    else:  # t-SNE\n",
        "        reducer = TSNE(n_components=2, random_state=42)\n",
        "\n",
        "    Z_2d = reducer.fit_transform(Z)\n",
        "\n",
        "    # Plot visualization\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot 2: Focus on specific digits\n",
        "    plt.subplot(1, 2, 2)\n",
        "    mask1 = y_test == digits[0]\n",
        "    mask2 = y_test == digits[1]\n",
        "\n",
        "    # write your code here\n",
        "\n",
        "    plt.title(f'Latent Space: Digits {digits[0]} vs {digits[1]}')\n",
        "\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return Z_2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjwJ0hiULHeV"
      },
      "source": [
        "## Hyperparemeter discussion\n",
        "You can try out changing hypermaters (leraning rate, hidden dim, num. of hidden layers) and report how that impact training loss, train/test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cMyVm8aU42f"
      },
      "source": [
        "## Question 2: Convolutional Neural Network (using PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJCYgZBomFPV"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqSoGKwGYUYL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math, random, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-3  # or 3e-4\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "validation_split = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRCdv3WSmsa1"
      },
      "source": [
        "\n",
        "## Part 1: Implementing the CNN\n",
        "\n",
        "You will build the core components of the CNN, with **both** forward and backward passes.\n",
        "\n",
        "### (a) Fully Connected Layer\n",
        "Implement a linear layer:\n",
        "y = xW^T + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywhrs-gTU_Aw"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias   = nn.Parameter(torch.Tensor(out_features)) if bias else None\n",
        "        self.reset_parameters()\n",
        "        self._cache_x = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Kaiming Uniform (fixed by assignment)\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Compute y = x W^T + b.\n",
        "        Hints:\n",
        "          • Use a batched matmul pattern that preserves leading batch dims.\n",
        "          • Add bias if present.\n",
        "        \"\"\"\n",
        "        self._cache_x = x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradients wrt input, weight, bias.\n",
        "        param grad_output: gradient wrt output\n",
        "        Return: (grad_input, grad_weight, grad_bias)\n",
        "        \"\"\"\n",
        "        x = self._cache_x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQxnt0Cny3v"
      },
      "source": [
        "### (b) Convolution Layer\n",
        "Implement a 2D convolution layer with basic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uupdLI22n9J_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomConv2D(nn.Module):\n",
        "    \"\"\"\n",
        "    2D Convolution (k x k) with manual forward/backward using unfold/fold.\n",
        "\n",
        "    Shapes and symbols:\n",
        "      x: input of shape (N, C_in, H, W)\n",
        "        N = batch size, C_in = input channels, H/W = spatial dims: Heigth/Width\n",
        "      w: weights of shape (C_out, C_in, k, k), with C_out = output channels\n",
        "      b: bias of shape (C_out,)\n",
        "      k: kernel_size\n",
        "      s: stride\n",
        "      p: padding\n",
        "\n",
        "      H_out = floor((H + 2*p - k)/s) + 1\n",
        "      W_out = floor((W + 2*p - k)/s) + 1\n",
        "      L = H_out * W_out   # number of sliding-window locations per feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias   = nn.Parameter(torch.Tensor(out_channels))\n",
        "        self.reset_parameters()\n",
        "        self._cache_x = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Kaiming Uniform (fixed)\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Compute conv output using F.unfold and a batched dot-product.\n",
        "        Hints:\n",
        "          • Unfold to shape (N, C_in*k*k, L) where L is number of locations.\n",
        "          • Reshape weights to (C_out, C_in*k*k).\n",
        "          • Add bias per output channel.\n",
        "          • Reshape to (N, C_out, H_out, W_out).\n",
        "        \"\"\"\n",
        "        self._cache_x = x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradients wrt input, weight, bias.\n",
        "        Hints:\n",
        "          • Reuse unfolded patches.\n",
        "          • grad_weight: correlate grad_output with unfolded input patches.\n",
        "          • grad_bias: sum grad_output over batch and spatial locations.\n",
        "          • grad_input: map grads back via an unfolded representation and fold.\n",
        "        Return: (grad_input, grad_weight, grad_bias)\n",
        "        \"\"\"\n",
        "        x = self._cache_x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ri_dFZ4onU-"
      },
      "source": [
        "### (c) Max Pooling Layer\n",
        "Implement a max pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPtvD4Bioyhb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomMaxPool2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Max Pooling (k x k) with manual forward/backward using unfold/fold.\n",
        "\n",
        "    Shapes and symbols:\n",
        "      x: input of shape (N, C, H, W)\n",
        "        N = batch size, C = channels, H/W = spatial dims: Height, Width\n",
        "      k: kernel_size (pool window is k x k)\n",
        "      s: stride\n",
        "      p: padding\n",
        "\n",
        "      H_out = floor((H + 2*p - k)/s) + 1\n",
        "      W_out = floor((W + 2*p - k)/s) + 1\n",
        "      L = H_out * W_out   # number of sliding-window locations per feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size, stride=None, padding=0):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride if stride is not None else kernel_size\n",
        "        self.padding = padding\n",
        "        self._cache_shape = None\n",
        "        self._cache_indices = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Max over k*k windows.\n",
        "        Hints:\n",
        "          • Unfold x → patches of shape (N, C*k*k, L)\n",
        "          • View → (N, C, k*k, L)\n",
        "          • Take max over the k*k dimension → outputs (N, C, L) and argmax indices (N, C, L)\n",
        "          • Cache argmax indices + shapes.\n",
        "          • Reshape output reshaped to (N, C, H_out, W_out).\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Implement max pooling backward (route grads to maxima only).\n",
        "        Inputs: grad_output: (N, C, H_out, W_out)\n",
        "        Hints:\n",
        "          • Make zeros tensor grad_unfold of shape (N, C, k*k, L)\n",
        "          • Scatter grad_output into grad_unfold at the argmax positions from forward\n",
        "          • View grad_unfold → (N, C*k*k, L)\n",
        "          • Fold back to image space → grad_input of shape (N, C, H, W)\n",
        "\n",
        "        Return: grad_input(N, C, H, W)\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6oBIYGJpP8_"
      },
      "source": [
        "### (d) Cross-Entropy Loss [2 points]\n",
        "Implement cross-entropy loss with numerical stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRiV60nKpU1s"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        self._cache = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        TODO: Compute numerically-stable cross-entropy.\n",
        "        Hints:\n",
        "          • Subtract max per row before exponentiation.\n",
        "          • Convert to probabilities; pick class probs at targets.\n",
        "          • Apply -log(...) and reduction (mean by default).\n",
        "          • Cache what's needed for backward.\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, logits=None, targets=None):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradient w.r.t. logits.\n",
        "        Hints:\n",
        "          • If `logits` and `targets` are provided, recompute softmax probs stably.\n",
        "            Otherwise, use cached probs/targets from forward().\n",
        "          • Divide by batch size if reduction == 'mean'.\n",
        "        Return: grad_logits\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FCQy3mhqz2v"
      },
      "source": [
        "\n",
        "## Part 2: Training the CNN\n",
        "\n",
        "Build and train the CNN for MNIST using **your custom layers**.\n",
        "\n",
        "### (a) Training pipeline code\n",
        "Address `TODO` in:\n",
        "1. **Backward pass section** — explicit chaining with your `backward()` methods.  \n",
        "2. **Parameter updates section** — manual updates using SGD-style with `torch.no_grad()`.\n",
        "\n",
        "### (b) Training and experiments\n",
        "Train your CNN on MNIST and run the following configurations:\n",
        "1. **Activations (2):** ReLU, Tanh.  \n",
        "2. **Learning rates (2):** 1e-3, 3e-4.\n",
        "\n",
        "Use consistent hyperparameters across runs. This yields 4 configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfmBUpKtrMj6"
      },
      "outputs": [],
      "source": [
        "def get_activation(name):\n",
        "    if name.lower() == \"relu\":\n",
        "        return F.relu, lambda z: (z > 0).to(z.dtype)\n",
        "    elif name.lower() == \"tanh\":\n",
        "        return torch.tanh, lambda z: 1 - torch.tanh(z)**2\n",
        "    else:\n",
        "        raise ValueError(\"Unknown activation: \" + name)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation_name=\"relu\", num_classes=10):\n",
        "        super().__init__()\n",
        "        self.ACT, self.ACT_PRIME = get_activation(activation_name)\n",
        "\n",
        "        self.conv1 = CustomConv2D(1, 32, 5)\n",
        "        self.conv2 = CustomConv2D(32, 64, 5)\n",
        "        self.max1  = CustomMaxPool2D(2, stride=2)\n",
        "        self.max2  = CustomMaxPool2D(2, stride=2)\n",
        "        self.fc1   = CustomLinear(64*4*4, 512)\n",
        "        self.fc2   = CustomLinear(512, num_classes)\n",
        "\n",
        "        self.criterion = CustomCrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.conv1(x)\n",
        "        a1 = self.ACT(z1)\n",
        "        p1 = self.max1(a1)\n",
        "\n",
        "        z2 = self.conv2(p1)\n",
        "        a2 = self.ACT(z2)\n",
        "        p2 = self.max2(a2)\n",
        "\n",
        "        flat = torch.flatten(p2, start_dim=1)\n",
        "        z3 = self.fc1(flat)\n",
        "        a3 = self.ACT(z3)\n",
        "        logits = self.fc2(a3)\n",
        "\n",
        "        # caches needed for manual backward\n",
        "        caches = (z1, a1, p1, z2, a2, p2, z3, a3)\n",
        "        return logits, caches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ez96avjrVBJ"
      },
      "source": [
        "### Data (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvbsKSkyrbjv"
      },
      "outputs": [],
      "source": [
        "\n",
        "full_train = datasets.MNIST(root=\"./data_CNN\", train=True, download=True, transform=ToTensor())\n",
        "test_ds    = datasets.MNIST(root=\"./data_CNN\", train=False, download=True, transform=ToTensor())\n",
        "\n",
        "train_size = int((1 - validation_split) * len(full_train))\n",
        "val_size   = len(full_train) - train_size\n",
        "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train/Val/Test sizes: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7eiVp0Pr-j-"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVCLP9mpsAuK"
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def accuracy_from_logits(logits, labels):\n",
        "    return (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "\n",
        "def sgd_update_(param, grad, lr):\n",
        "    param -= lr * grad\n",
        "    return param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7gkJ1mVsJbh"
      },
      "source": [
        "\n",
        "### Training Pipeline code\n",
        "Address the TODOs in the code below\n",
        "> - Complete the BACKWARD PASS (explicit chaining with your `.backward` methods).\n",
        "> - Uncomment the PARAMETER UPDATES (manual SGD with no_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3oQ2mxPtT8V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_one_config(activation_name=\"relu\", epochs=num_epochs, lr=learning_rate, num_classes=num_classes):\n",
        "    model = CNN(activation_name=activation_name, num_classes=num_classes).to(device)\n",
        "    criterion = model.criterion\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs,   val_accs   = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss, total_train_correct, total_train_examples = 0.0, 0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            # ---------- Forward ----------\n",
        "            logits, caches = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "\n",
        "            # Unpack caches (pre/post activations)\n",
        "            z1, a1, p1, d1, z2, a2, p2, d2, z3, a3 = caches\n",
        "\n",
        "            # ==========================================================================\n",
        "            # BACKWARD PASS (explicit chaining with your .backward methods)\n",
        "            # TODO: Implement the manual chain below and produce gradients with names:\n",
        "            #       grad_fc2_w, grad_fc2_b, grad_fc1_w, grad_fc1_b,\n",
        "            #       grad_conv2_w, grad_conv2_b, grad_conv1_w, grad_conv1_b\n",
        "            #\n",
        "            # Hints:\n",
        "            #   1- Check PARAMETER UPDATES codes below for ideas and variable names\n",
        "            #   2- Chain in the following way:\n",
        "            #       - dL/dlogits from Cross-Entropy Loss\n",
        "            #       - FC2 backward\n",
        "            #       - Activation after FC1\n",
        "            #       - FC1 backward\n",
        "            #       - Reshape to feature map\n",
        "            #       - MaxPool2 backward\n",
        "            #       - Activation after Conv2\n",
        "            #       - Conv2 backward\n",
        "            #       - MaxPool1 backward\n",
        "            #       - Activation after Conv1\n",
        "            #       - Conv1 backward\n",
        "            \"WRITE YOUR CODE HERE\"\n",
        "\n",
        "\n",
        "            # ===========================================\n",
        "            # PARAMETER UPDATES (manual SGD with no_grad)\n",
        "            # TODO: UNCOMMENT THE CODES BELOW, DO NOT CHANGE\n",
        "            # ===========================================\n",
        "            # with torch.no_grad():\n",
        "            #     # FC2\n",
        "            #     model.fc2.weight.copy_(sgd_update_(model.fc2.weight, grad_fc2_w, lr))\n",
        "            #     if model.fc2.bias is not None:\n",
        "            #         model.fc2.bias.copy_(sgd_update_(model.fc2.bias, grad_fc2_b, lr))\n",
        "            #     # FC1\n",
        "            #     model.fc1.weight.copy_(sgd_update_(model.fc1.weight, grad_fc1_w, lr))\n",
        "            #     if model.fc1.bias is not None:\n",
        "            #         model.fc1.bias.copy_(sgd_update_(model.fc1.bias, grad_fc1_b, lr))\n",
        "            #     # Conv2\n",
        "            #     model.conv2.weight.copy_(sgd_update_(model.conv2.weight, grad_conv2_w, lr))\n",
        "            #     model.conv2.bias.copy_(sgd_update_(model.conv2.bias, grad_conv2_b, lr))\n",
        "            #     # Conv1\n",
        "            #     model.conv1.weight.copy_(sgd_update_(model.conv1.weight, grad_conv1_w, lr))\n",
        "            #     model.conv1.bias.copy_(sgd_update_(model.conv1.bias, grad_conv1_b, lr))\n",
        "\n",
        "\n",
        "            # ---------- Train metrics ----------\n",
        "            with torch.no_grad():\n",
        "                total_train_loss += loss.item()\n",
        "                total_train_examples += yb.size(0)\n",
        "                total_train_correct += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        train_losses.append(total_train_loss / len(train_loader))\n",
        "        train_accs.append(total_train_correct / total_train_examples)\n",
        "\n",
        "        # ---------- Validation ----------\n",
        "        model.eval()\n",
        "        val_loss_sum, val_correct, val_examples = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits, _ = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_loss_sum += loss.item()\n",
        "                val_examples += yb.size(0)\n",
        "                val_correct += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss_sum / len(val_loader))\n",
        "        val_accs.append(val_correct / val_examples)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d} | \"\n",
        "              f\"Train Loss {train_losses[-1]:.4f}  Acc {train_accs[-1]*100:.2f}% | \"\n",
        "              f\"Val Loss {val_losses[-1]:.4f}  Acc {val_accs[-1]*100:.2f}%\")\n",
        "\n",
        "    # ---------- Test ----------\n",
        "    model.eval()\n",
        "    test_correct, test_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits, _ = model(xb)\n",
        "            test_correct += (logits.argmax(1) == yb).sum().item()\n",
        "            test_total += yb.size(0)\n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": train_losses, \"val_loss\": val_losses,\n",
        "        \"train_acc\": train_accs,   \"val_acc\": val_accs,\n",
        "        \"test_acc\": test_acc\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5cGWM7Rv1X2"
      },
      "source": [
        "### Training, and experiments\n",
        "Train your CNN on MNIST and run the following configurations:\n",
        "1. **Activations (2):** ReLU, Tanh.  \n",
        "2. **Learning rates (2):** 1e-3, 3e-4.\n",
        "\n",
        "Use consistent hyperparameters across runs. This yields 4 configurations:\n",
        "- ReLU / LR = 1e-3\n",
        "- ReLU / LR = 3e-4\n",
        "- Tanh / LR = 1e-3\n",
        "- Tanh / LR = 3e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10i1TyZowgSe"
      },
      "outputs": [],
      "source": [
        "# Training, and experiments : 4 configurations in total\n",
        "\"WRITE YOUR CODE HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqKoezEJxIXb"
      },
      "source": [
        "\n",
        "## Part 3: Results and Discussions\n",
        "\n",
        "1. **Plots:** Submit clear plots of training and validation **accuracy** and **loss** for each configuration. In addition, include the **test-set accuracy** on these plots. **Discuss results for each configuration**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSMcfgOqx-qS"
      },
      "outputs": [],
      "source": [
        "# Plots\n",
        "\"WRITE YOUR CODE HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj3W2K4yxVtY"
      },
      "source": [
        "  \n",
        "2. **Write a short discussion:** (~1 page) reflecting on:\n",
        "   - Which **activations/learning rates** choices performed best and why?\n",
        "   - Any signs of over/underfitting you observed.\n",
        "\n",
        "Note: Please upload the discussion write-ups of all sections of **Part 3** as a single PDF file on Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiBviemUx4Kc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aims",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
