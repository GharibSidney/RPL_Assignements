{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyJ4Y-50zfCt"
      },
      "source": [
        "# Notes: You will have to:\n",
        "  >1- Make a Copy of this notebook to edit it for your solutions;\n",
        "  >\n",
        "  >2- Upload on Gradescope: The `Colab Notebook edited with your solutions`, and a `pdf Report (Discussion questions)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNWoiYrUTh5q"
      },
      "source": [
        "# Question 1: MLP Implementation with Numpy\n",
        "\n",
        "In this exercise, we will explore the construction of a multi-layer perceptron (MLP) by coding it from the ground up. We will develop a deep neural network step-by-step, beginning with a single neuron, progressing to a layer, and ultimately building the entire network. We will then train this network and evaulate on an interesting dataset and try to understand the workings in more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJdB1Yv4T25Z"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ocuxjx05TKdn"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3idgvI3_T-dC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgDzUKUNUK_n"
      },
      "source": [
        "## Q 1.1 (a): Activation Functions\n",
        "\n",
        "In this question you will implement the softmax and ReLU activation functions and their derivatives. Both these functions help introduce non-linearity which gives neural network their capacity. We will implement these functions in the `ActivationFunction` class as static methods. You can access the method later using the following code:\n",
        "\n",
        "```\n",
        "# For softmax activation\n",
        "ActivationFunction.softmax(x)\n",
        "\n",
        "# For calucating the derivative of softmax function\n",
        "ActivationFunction.softmax_derivative(x)\n",
        "\n",
        "# For ReLU activation\n",
        "ActivationFunction.relu(x)\n",
        "\n",
        "# For calculating the derivative of ReLU function\n",
        "ActivationFunction.relu_derivative(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZLkQNPWULZe"
      },
      "outputs": [],
      "source": [
        "class ActivationFunction:\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        \"\"\"\n",
        "        Compute the Softmax activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Softmax activation values\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        if x.ndim == 0:\n",
        "            return np.array(1.0, dtype=float)\n",
        "        if x.ndim == 1:\n",
        "            return np.exp(x) / (np.sum(np.exp(x)))\n",
        "        else:\n",
        "            return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the Softmax function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Derivative of Softmax function\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "\n",
        "        exps = np.exp(x - np.max(x))  \n",
        "        s = exps / np.sum(exps)\n",
        "\n",
        "        return  np.diag(s) - np.outer(s, s)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        \"\"\"\n",
        "        Compute the ReLU activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: ReLU activation values\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(x):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the ReLU activation function.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input values\n",
        "\n",
        "        Returns:\n",
        "            np.array: Derivative of ReLU function\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77bPGi4UVCH"
      },
      "source": [
        "## Q 1.1 (b): Neuron Implementation\n",
        "\n",
        "Neuron is the fundamental building block of MLP. A neuron simulates a biological neuron which can be activated or fired when responding to a stimulus. The equation of a neuron is as follow:\n",
        "\n",
        "$$ f(x) = a(\\mathbf{w} \\cdot \\mathbf{x} + b) $$\n",
        "\n",
        "where inputs $\\mathbf{x}$ and weights $\\mathbf{w}$ are multidimensional vectors, bias $b$ is a scalar, $a(.)$ is an activation function.\n",
        "\n",
        "**Note:** For the whole exercise you will be receiving inputs of batch size of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaME5ZMeUYBz"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, input_size, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize a neuron with random weights and bias.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "            activation_function (function): Activation function to use\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        self.activation_function = activation_function\n",
        "        self.weights = np.random.randn(input_size)\n",
        "        self.bias = np.random.randn()\n",
        "\n",
        "    def activate(self, x):\n",
        "        \"\"\"\n",
        "        Compute the activation of the neuron.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            float: Activation value\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return self.activation_function(self.weights @ x + self.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7nfeRx3UdPQ"
      },
      "source": [
        "## Q 1.1(c): Layer Implementation\n",
        "\n",
        "Now we will compose a layer which consists of multiple neurons. Complete initializing the layer and compute the forward pass by sending the input through each neuron of the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzrfOWC1UZ-m"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize a layer of neurons.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "            output_size (int): Number of neurons in the layer\n",
        "            activation_function (function): Activation function to use\n",
        "        \"\"\"\n",
        "        ## Write your code here ##\n",
        "        self.neurons = [Neuron(input_size, activation_function) for _ in range(output_size)]\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the forward pass through the layer.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.array: Output of the layer\n",
        "        \"\"\"\n",
        "        ### Write your code here ###\n",
        "        return np.array([neuron.activate(x) for neuron in self.neurons])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeUxOv7kUh5z"
      },
      "source": [
        "## Q 1.1(d) + 2 (a) to (d): Neural Network Construction\n",
        "\n",
        "Now we have built all the building blocks required to build our neural network. Complete the implementations using the hints given in the code. There are additional functions that will be useful later. Do not modify these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeVTOZ_uUjrF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, hidden_dim, activation_function_hidden, activation_output):\n",
        "        \"\"\"\n",
        "        Initialize a neural network with specified layer sizes.\n",
        "\n",
        "        Args:\n",
        "            layer_sizes (list): List of integers representing the size of each layer\n",
        "            hidden_dim (int): Dimension of hidden layers\n",
        "            activation_function_hidden (function): Activation function for hidden layers\n",
        "            activation_output (function): Activation function for output layer\n",
        "        \"\"\"\n",
        "        self.layers = []\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.activation_function_hidden = activation_function_hidden\n",
        "        self.activation_output = activation_output\n",
        "        \n",
        "        # Build layers according to layer_sizes\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            input_size = layer_sizes[i]\n",
        "            output_size = layer_sizes[i + 1]\n",
        "            if i < len(layer_sizes) - 2:\n",
        "                activation = activation_function_hidden\n",
        "            else:\n",
        "                 activation = lambda x: x \n",
        "            self.layers.append(Layer(input_size, output_size, activation))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute forward pass through network.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input features\n",
        "\n",
        "        Returns:\n",
        "            np.array: Output probabilities after softmax\n",
        "        \"\"\"\n",
        "        output = x\n",
        "        self.activations = [x]\n",
        "        self.pre_activations = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "            self.activations.append(output)\n",
        "\n",
        "        z = output  \n",
        "        self.pre_activations.append(z)\n",
        "\n",
        "        exp_logits = np.exp(z - np.max(z))\n",
        "        softmax_output = exp_logits / np.sum(exp_logits)\n",
        "        self.activations[-1] = softmax_output\n",
        "\n",
        "        return softmax_output\n",
        "\n",
        "    def layer_backward(self, delta, layer_index, activations, zs):\n",
        "        \"\"\"\n",
        "        Backpropagation for a single layer.\n",
        "\n",
        "        Args:\n",
        "            delta (np.array): gradient of loss wrt current layer output (shape: layer_size)\n",
        "            layer_index (int): index of current layer\n",
        "            activations (list): list of activations from forward pass\n",
        "            zs (list): list of pre-activation (z) values from forward pass\n",
        "\n",
        "        Returns:\n",
        "            tuple: (delta for previous layer, gradient_w, gradient_b)\n",
        "        \"\"\"\n",
        "        layer = self.layers[layer_index]\n",
        "        z = zs[layer_index]\n",
        "        a_prev = activations[layer_index]  # activation of previous layer\n",
        "\n",
        "        if layer_index == len(self.layers) - 1:\n",
        "            # output layer with softmax + cross-entropy: delta is already (output - target)\n",
        "            pass\n",
        "        else:\n",
        "            # hidden layers with ReLU activation\n",
        "            if self.activation_function_hidden == ActivationFunction.relu:\n",
        "                delta = delta * ActivationFunction.relu_derivative(z)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "        gradient_w = np.outer(delta, a_prev)  # shape: (current_layer_size, previous_layer_size)\n",
        "        gradient_b = delta  # shape: (current_layer_size,)\n",
        "\n",
        "        weights = np.array([neuron.weights for neuron in layer.neurons])  # shape: (current_layer_size, previous_layer_size)\n",
        "        delta_prev = np.dot(weights.T, delta)  # backpropagate delta to previous layer\n",
        "\n",
        "        return delta_prev, gradient_w, gradient_b\n",
        "\n",
        "\n",
        "    def backward(self, x, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform backpropagation and update weights.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): input features\n",
        "            y (np.array): true one-hot labels\n",
        "            learning_rate (float): learning rate\n",
        "        \"\"\"\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for layer in self.layers:\n",
        "            z = np.array([np.dot(neuron.weights, activations[-1]) + neuron.bias for neuron in layer.neurons])\n",
        "            zs.append(z)\n",
        "            activation = layer.forward(activations[-1])\n",
        "            activations.append(activation)\n",
        "\n",
        "        # delta at output layer: output - target (for softmax + cross-entropy)\n",
        "        delta = activations[-1] - y\n",
        "\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            delta, gradient_w, gradient_b = self.layer_backward(delta, i, activations, zs)\n",
        "\n",
        "            for j, neuron in enumerate(self.layers[i].neurons):\n",
        "                neuron.weights -= learning_rate * gradient_w[j]\n",
        "                neuron.bias -= learning_rate * gradient_b[j]\n",
        "\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): training features (num_samples, input_size)\n",
        "            y (np.array): one-hot encoded labels (num_samples, num_classes)\n",
        "            epochs (int): number of training epochs\n",
        "            learning_rate (float): learning rate\n",
        "        \"\"\"\n",
        "        train_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            predictions = []\n",
        "\n",
        "            for i in range(len(X)):\n",
        "                x_sample = X[i]\n",
        "                y_sample = y[i]\n",
        "\n",
        "                prediction = self.forward(x_sample)\n",
        "                predictions.append(prediction)\n",
        "\n",
        "                self.backward(x_sample, y_sample, learning_rate)\n",
        "\n",
        "            predictions = np.array(predictions)\n",
        "            avg_loss = self.compute_loss(predictions, y)\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "            \n",
        "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return train_losses\n",
        "\n",
        "\n",
        "    def compute_loss(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Compute average cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            predictions (np.array): predicted softmax probabilities (batch_size, num_classes)\n",
        "            targets (np.array): one-hot encoded true labels (batch_size, num_classes)\n",
        "\n",
        "        Returns:\n",
        "            float: average cross-entropy loss\n",
        "        \"\"\"\n",
        "        epsilon = 1e-9\n",
        "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
        "        loss = -np.sum(targets * np.log(predictions)) / predictions.shape[0]\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions for a batch of inputs.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): input features (num_samples, input_size)\n",
        "\n",
        "        Returns:\n",
        "            np.array: predictions (num_samples, num_classes)\n",
        "        \"\"\"\n",
        "        return np.array([self.forward(x) for x in X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JicJ29ryUlDS"
      },
      "source": [
        "# Q 1.2(d): Training and Visualizing Neural Network\n",
        "\n",
        "In the rest of the exercise we will use the components we built earlier to train a neural network and visualise the loss curve. To make things simple we will be building neural networks with only one hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vO7ogsLUnP2"
      },
      "source": [
        "We will use the MNIST dataset for all our experiements which consists of 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gYj9nvIUqyr"
      },
      "outputs": [],
      "source": [
        "# # Uncomment the below code to create the dataset. And comment it when submitting to gradescope\n",
        "\n",
        "# np.random.seed(42) # Do not change\n",
        "# # For easy data loading and processing, we're using torch here. You're not allowed to use torch for Neural Net components though.\n",
        "# def get_train_test_dataset():\n",
        "#     import torch\n",
        "#     from torchvision import datasets, transforms\n",
        "#     from sklearn.model_selection import train_test_split\n",
        "\n",
        "#     # Define a transform to convert images to tensors and normalize\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.ToTensor()\n",
        "#     ])\n",
        "\n",
        "#     # Load the MNIST dataset\n",
        "#     mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "#     # Take a subset of 1000 samples\n",
        "#     X = mnist_dataset.data[:1000].float() / 255.0  # Normalize pixel values to [0,1]\n",
        "#     y = mnist_dataset.targets[:1000]\n",
        "\n",
        "#     # Convert X to numpy arrays and flatten images from 28x28 to 784 features\n",
        "#     X = X.numpy().reshape(-1, 28*28)\n",
        "#     y = y.numpy()\n",
        "\n",
        "#     # Split dataset into training and test sets (e.g., 80% train, 20% test)\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "#     print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "#     print(\"Test set shape :\", X_test.shape, y_test.shape)\n",
        "\n",
        "#     return X_train, y_train, X_test, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSq1qSzQFPy0"
      },
      "source": [
        "## Let's visualize a few samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "t1nM7oF6FNJd",
        "outputId": "1eff200a-38b2-4d91-951e-cdb6ba395f3c"
      },
      "outputs": [],
      "source": [
        "# Plot a few images\n",
        "\n",
        "def visualize_mnist_samples(X, y, num_samples=9):\n",
        "    \"\"\"Visualize MNIST sample images\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(min(num_samples, len(X))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        # Reshape from flattened to 28x28 for visualization\n",
        "        img = X[i].reshape(28, 28)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Label: {y[i]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle('MNIST Sample Images')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "X_train, y_train, X_test, y_test =  get_train_test_dataset()\n",
        "visualize_mnist_samples(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTVzbcSUugW"
      },
      "source": [
        "Below you have been given the code to train your neural network. and visualize the decision boundry. Modify the hyperparameters to train and visualize the neural network and report your findings in the practical report. We will be testing our model on the training set. The `plot_decision_boundary` method takes the data created earlier and the neural network you trained to show the decision boundry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGqDsjU7Uu6b"
      },
      "outputs": [],
      "source": [
        "def train_network(hidden_layer_size=1, hidden_dim=64,\n",
        "                  epochs=10, activation_hidden=np.tanh,\n",
        "                  activation_output=None, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Trains a neural network on MNIST and evaluates performance.\n",
        "\n",
        "    Parameters:\n",
        "        X_train, y_train: Training data (images and labels)\n",
        "        X_test, y_test: Test data (for evaluation)\n",
        "        hidden_layer_size: Number of hidden layers\n",
        "        hidden_dim: Number of neurons per hidden layer\n",
        "        epochs: Training epochs\n",
        "        activation_hidden: Activation function for hidden layers\n",
        "        activation_output: Activation for output (e.g., softmax)\n",
        "        learning_rate: Learning rate\n",
        "\n",
        "    Returns:\n",
        "        nn: Trained neural network object\n",
        "        train_loss: Training loss history\n",
        "        train_accuracy: Accuracy on training set\n",
        "        test_accuracy: Accuracy on test set\n",
        "    \"\"\"\n",
        "    X_train, y_train, X_test, y_test = get_train_test_dataset()\n",
        "    input_size, output_size = 28 * 28, 10\n",
        "    layers = [input_size] + [hidden_dim] * hidden_layer_size + [output_size]\n",
        "\n",
        "    # Initialize network\n",
        "    nn = NeuralNetwork(layers, hidden_dim, activation_hidden, activation_output)\n",
        "\n",
        "\n",
        "    # --- Prepare training data ---\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1) / 255.0  # Normalize\n",
        "    y_train_np = y_train.numpy() if hasattr(y_train, \"numpy\") else y_train\n",
        "    y_train_onehot = np.eye(output_size)[y_train_np]  # One-hot encode\n",
        "\n",
        "    # --- Train the model ---\n",
        "    train_loss = nn.train(X_train_flat, y_train_onehot,\n",
        "                          epochs=epochs, learning_rate=learning_rate)\n",
        "\n",
        "    # --- Evaluate on training data ---\n",
        "    preds_train = np.array([nn.forward(x) for x in X_train_flat])\n",
        "\n",
        "    preds_train_labels = np.argmax(preds_train, axis=1)\n",
        "    train_accuracy = np.mean(preds_train_labels == y_train_np)\n",
        "\n",
        "    # --- Evaluate on test data ---\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "    y_test_np = y_test.numpy() if hasattr(y_test, \"numpy\") else y_test\n",
        "    preds_test = np.array([nn.forward(x) for x in X_test_flat])\n",
        "    preds_test_labels = np.argmax(preds_test, axis=1)\n",
        "    test_accuracy = np.mean(preds_test_labels == y_test_np)\n",
        "\n",
        "    return nn, train_loss, train_accuracy, test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmQowEJgUySb"
      },
      "outputs": [],
      "source": [
        "# # Write the code to train and visualize\n",
        "# # Consider commenting this out when submitting to gradescope\n",
        "\n",
        "# # --- Set Hyperparameters ---\n",
        "# hidden_layer_size = 2            # Number of hidden layers\n",
        "# hidden_dim = 64                  # Neurons per hidden layer\n",
        "# epochs = 20                      # Training epochs\n",
        "# activation_hidden = ActivationFunction.relu     # Hidden layer activation\n",
        "# activation_output = ActivationFunction.softmax         # Let the loss function handle softmax internally\n",
        "# learning_rate = 0.0001             # Learning rate\n",
        "\n",
        "# # --- Train the network ---\n",
        "# nn, train_loss, train_accuracy, test_accuracy = train_network(\n",
        "#     hidden_layer_size=hidden_layer_size,\n",
        "#     hidden_dim=hidden_dim,\n",
        "#     epochs=epochs,\n",
        "#     activation_hidden=activation_hidden,\n",
        "#     activation_output=activation_output,\n",
        "#     learning_rate=learning_rate\n",
        "# )\n",
        "\n",
        "# print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "# print(f\"Test Accuracy:  {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mGVTWsJRpuu"
      },
      "source": [
        "# Q 1.3: Reporting and analysis\n",
        "Analyze your trained model and present findings clearly as asked in the instruction PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adETdDEQR0va"
      },
      "outputs": [],
      "source": [
        "print(\"Train accuracy: \", train_accuracy)\n",
        "print(\"Test accuracy: \", test_accuracy)\n",
        "plt.plot(range(1, epochs + 1), train_loss, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss per Epoch')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVHN2rRqSEOh"
      },
      "source": [
        "# Training loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RorfYPUfU2sI"
      },
      "outputs": [],
      "source": [
        "# Plot the training loss across 20 epochs\n",
        "\n",
        "def plot_loss_curve(train_loss):\n",
        "  # Write your code here\n",
        "  epochs = range(1, len(train_loss) + 1)\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(epochs, train_loss, marker='o', linestyle='-', color='b')\n",
        "  plt.title(\"Training Loss Across Epochs\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7V5IzafMXJF"
      },
      "source": [
        "## Layer activation visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1EaFVfDMb3u"
      },
      "outputs": [],
      "source": [
        "# ===== Latent Space Visualization =====\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def get_penultimate_activations(nn, X_batch):\n",
        "    \"\"\"Get activations from the last hidden layer\"\"\"\n",
        "    # Process each sample individually\n",
        "    activations_list = []\n",
        "    for x in X_batch:\n",
        "        activations = x\n",
        "        for layer in nn.layers[:-1]:  # exclude final output layer\n",
        "            activations = layer.forward(activations)\n",
        "        activations_list.append(activations)\n",
        "    return np.array(activations_list)\n",
        "\n",
        "def visualize_activations(nn, X_test, y_test, digits=(3, 8), method='PCA'):\n",
        "    \"\"\"Visualize layer activations using PCA or t-SNE\"\"\"\n",
        "    # Get activations from penultimate layer\n",
        "    Z = get_penultimate_activations(nn, X_test)\n",
        "\n",
        "    # Apply dimensionality reduction\n",
        "    if method == 'PCA':\n",
        "        reducer = PCA(n_components=2)\n",
        "    else:  # t-SNE\n",
        "        reducer = TSNE(n_components=2, random_state=42)\n",
        "\n",
        "    Z_2d = reducer.fit_transform(Z)\n",
        "\n",
        "    # Plot visualization\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot 2: Focus on specific digits\n",
        "    plt.subplot(1, 2, 2)\n",
        "    mask1 = y_test == digits[0]\n",
        "    mask2 = y_test == digits[1]\n",
        "\n",
        "    # write your code here\n",
        "\n",
        "    plt.title(f'Latent Space: Digits {digits[0]} vs {digits[1]}')\n",
        "\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return Z_2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjwJ0hiULHeV"
      },
      "source": [
        "## Hyperparemeter discussion\n",
        "You can try out changing hypermaters (leraning rate, hidden dim, num. of hidden layers) and report how that impact training loss, train/test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cMyVm8aU42f"
      },
      "source": [
        "## Question 2: Convolutional Neural Network (using PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJCYgZBomFPV"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqSoGKwGYUYL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math, random, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-3  # or 3e-4\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "validation_split = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRCdv3WSmsa1"
      },
      "source": [
        "\n",
        "## Part 1: Implementing the CNN\n",
        "\n",
        "You will build the core components of the CNN, with **both** forward and backward passes.\n",
        "\n",
        "### (a) Fully Connected Layer\n",
        "Implement a linear layer:\n",
        "y = xW^T + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywhrs-gTU_Aw"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias   = nn.Parameter(torch.Tensor(out_features)) if bias else None\n",
        "        self.reset_parameters()\n",
        "        self._cache_x = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Kaiming Uniform (fixed by assignment)\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Compute y = x W^T + b.\n",
        "        Hints:\n",
        "          • Use a batched matmul pattern that preserves leading batch dims.\n",
        "          • Add bias if present.\n",
        "        \"\"\"\n",
        "        self._cache_x = x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        output = x @ self.weight.T  # Matrix multiply\n",
        "        if self.bias is not None:\n",
        "            output += self.bias  # Add bias (broadcasted over batch)\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradients wrt input, weight, bias.\n",
        "        param grad_output: gradient wrt output\n",
        "        Return: (grad_input, grad_weight, grad_bias)\n",
        "        \"\"\"\n",
        "        x = self._cache_x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        grad_input = grad_output @ self.weight \n",
        "\n",
        "        # Gradient w.r.t weights: dL/dW = grad_output^T @ x\n",
        "        grad_weight = grad_output.T @ x\n",
        "\n",
        "        grad_bias = grad_output.sum(dim=0) if self.bias is not None else None\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQxnt0Cny3v"
      },
      "source": [
        "### (b) Convolution Layer\n",
        "Implement a 2D convolution layer with basic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uupdLI22n9J_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomConv2D(nn.Module):\n",
        "    \"\"\"\n",
        "    2D Convolution (k x k) with manual forward/backward using unfold/fold.\n",
        "\n",
        "    Shapes and symbols:\n",
        "      x: input of shape (N, C_in, H, W)\n",
        "        N = batch size, C_in = input channels, H/W = spatial dims: Heigth/Width\n",
        "      w: weights of shape (C_out, C_in, k, k), with C_out = output channels\n",
        "      b: bias of shape (C_out,)\n",
        "      k: kernel_size\n",
        "      s: stride\n",
        "      p: padding\n",
        "\n",
        "      H_out = floor((H + 2*p - k)/s) + 1\n",
        "      W_out = floor((W + 2*p - k)/s) + 1\n",
        "      L = H_out * W_out   # number of sliding-window locations per feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias   = nn.Parameter(torch.Tensor(out_channels))\n",
        "        self.reset_parameters()\n",
        "        self._cache_x = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Kaiming Uniform (fixed)\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Compute conv output using F.unfold and a batched dot-product.\n",
        "        Hints:\n",
        "          • Unfold to shape (N, C_in*k*k, L) where L is number of locations.\n",
        "          • Reshape weights to (C_out, C_in*k*k).\n",
        "          • Add bias per output channel.\n",
        "          • Reshape to (N, C_out, H_out, W_out).\n",
        "        \"\"\"\n",
        "        self._cache_x = x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        N, C_in, H, W = x.shape\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        p = self.padding\n",
        "\n",
        "        x_unfold = F.unfold(x, kernel_size=k, stride=s, padding=p)\n",
        "        self._cache_x = x\n",
        "        self._cache_x_unfold = x_unfold\n",
        "\n",
        "        weight_flat = self.weight.view(self.out_channels, -1)  # (C_out, C_in * k * k)\n",
        "\n",
        "        # Correct matmul: (N, L, C_in*k*k) @ (C_in*k*k, C_out).T = (N, L, C_out)\n",
        "        out = x_unfold.transpose(1, 2) @ weight_flat.T\n",
        "        out = out.transpose(1, 2)  # (N, C_out, L)\n",
        "\n",
        "        out += self.bias.view(1, -1, 1)  # (1, C_out, 1)\n",
        "\n",
        "\n",
        "        H_out = (H + 2 * p - k) // s + 1\n",
        "        W_out = (W + 2 * p - k) // s + 1\n",
        "        self._output_shape = (H_out, W_out)\n",
        "\n",
        "        # Reshape to (N, C_out, H_out, W_out)\n",
        "        return out.view(N, self.out_channels, H_out, W_out)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradients wrt input, weight, bias.\n",
        "        Hints:\n",
        "          • Reuse unfolded patches.\n",
        "          • grad_weight: correlate grad_output with unfolded input patches.\n",
        "          • grad_bias: sum grad_output over batch and spatial locations.\n",
        "          • grad_input: map grads back via an unfolded representation and fold.\n",
        "        Return: (grad_input, grad_weight, grad_bias)\n",
        "        \"\"\"\n",
        "        x = self._cache_x\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        x_unfold = self._cache_x_unfold  # (N, in_channels * k^2, L)\n",
        "        H_out, W_out = self._output_shape\n",
        "        N = grad_output.shape[0]\n",
        "\n",
        "        grad_output_flat = grad_output.view(N, self.out_channels, -1)  # (N, out_channels, L)\n",
        "        weight_flat = self.weight.view(self.out_channels, -1)  # (out_channels, in_channels * k^2)\n",
        "\n",
        "        # grad_weight: correlate grad_output with input patches\n",
        "        # grad_output_flat: (N, out_channels, L)\n",
        "        # x_unfold: (N, in_channels*k^2, L)\n",
        "        grad_weight = torch.bmm(grad_output_flat, x_unfold.permute(0, 2, 1))  # (N, out_channels, in_channels*k^2)\n",
        "        grad_weight = grad_weight.sum(dim=0)  # sum over batch, shape (out_channels, in_channels*k^2)\n",
        "        grad_weight = grad_weight.view_as(self.weight)  # reshape to weight shape\n",
        "\n",
        "        # grad_bias: sum over batch and spatial locations\n",
        "        grad_bias = grad_output_flat.sum(dim=(0, 2)) if self.bias is not None else None  # shape (out_channels,)\n",
        "\n",
        "        # grad_input (already computed)\n",
        "        grad_input_unfold = torch.bmm(weight_flat.t().unsqueeze(0).expand(N, -1, -1), grad_output_flat)  # (N, in_channels*k^2, L)\n",
        "\n",
        "\n",
        "        grad_input = F.fold(\n",
        "            grad_input_unfold,\n",
        "            output_size=x.shape[2:],  # original H, W of input\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding\n",
        "        )\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ri_dFZ4onU-"
      },
      "source": [
        "### (c) Max Pooling Layer\n",
        "Implement a max pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPtvD4Bioyhb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomMaxPool2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Max Pooling (k x k) with manual forward/backward using unfold/fold.\n",
        "\n",
        "    Shapes and symbols:\n",
        "      x: input of shape (N, C, H, W)\n",
        "        N = batch size, C = channels, H/W = spatial dims: Height, Width\n",
        "      k: kernel_size (pool window is k x k)\n",
        "      s: stride\n",
        "      p: padding\n",
        "\n",
        "      H_out = floor((H + 2*p - k)/s) + 1\n",
        "      W_out = floor((W + 2*p - k)/s) + 1\n",
        "      L = H_out * W_out   # number of sliding-window locations per feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size, stride=None, padding=0):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride if stride is not None else kernel_size\n",
        "        self.padding = padding\n",
        "        self._cache_shape = None\n",
        "        self._cache_indices = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Max over k*k windows.\n",
        "        Hints:\n",
        "          • Unfold x → patches of shape (N, C*k*k, L)\n",
        "          • View → (N, C, k*k, L)\n",
        "          • Take max over the k*k dimension → outputs (N, C, L) and argmax indices (N, C, L)\n",
        "          • Cache argmax indices + shapes.\n",
        "          • Reshape output reshaped to (N, C, H_out, W_out).\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        N, C, H, W = x.shape\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        p = self.padding\n",
        "\n",
        "        self._cache_shape = x.shape\n",
        "        x_padded = F.pad(x, (p, p, p, p))\n",
        "        x_unfolded = F.unfold(x_padded, kernel_size=k, stride=s)\n",
        "        x_unfolded = x_unfolded.view(N, C, k * k, -1)\n",
        "        out, indices = x_unfolded.max(dim=2)  # (N, C, L)\n",
        "\n",
        "        self._cache_indices = indices\n",
        "\n",
        "        # Compute H_out and W_out\n",
        "        H_out = (H + 2 * p - k) // s + 1\n",
        "        W_out = (W + 2 * p - k) // s + 1\n",
        "\n",
        "        # Reshape output to (N, C, H_out, W_out)\n",
        "        return out.view(N, C, H_out, W_out)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        TODO: Implement max pooling backward (route grads to maxima only).\n",
        "        Inputs: grad_output: (N, C, H_out, W_out)\n",
        "        Hints:\n",
        "          • Make zeros tensor grad_unfold of shape (N, C, k*k, L)\n",
        "          • Scatter grad_output into grad_unfold at the argmax positions from forward\n",
        "          • View grad_unfold → (N, C*k*k, L)\n",
        "          • Fold back to image space → grad_input of shape (N, C, H, W)\n",
        "\n",
        "        Return: grad_input(N, C, H, W)\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        N, C, H_out, W_out = grad_output.shape\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        p = self.padding\n",
        "        H, W = self._cache_shape[2:]\n",
        "\n",
        "        grad_output_flat = grad_output.view(N, C, -1)\n",
        "        L = grad_output_flat.shape[-1]\n",
        "\n",
        "        grad_unfold = torch.zeros(N, C, k * k, L, device=grad_output.device)\n",
        "        grad_unfold.scatter_(2, self._cache_indices.unsqueeze(2), grad_output_flat.unsqueeze(2))\n",
        "        grad_unfold = grad_unfold.view(N, C * k * k, L)\n",
        "\n",
        "        grad_input_padded = F.fold(grad_unfold, \n",
        "                                   output_size=(H + 2 * p, W + 2 * p),\n",
        "                                   kernel_size=k,\n",
        "                                   stride=s)\n",
        "\n",
        "        # Remove padding\n",
        "        if p > 0:\n",
        "            grad_input = grad_input_padded[:, :, p:-p, p:-p]\n",
        "        else:\n",
        "            grad_input = grad_input_padded\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6oBIYGJpP8_"
      },
      "source": [
        "### (d) Cross-Entropy Loss [2 points]\n",
        "Implement cross-entropy loss with numerical stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRiV60nKpU1s"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        self._cache = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        TODO: Compute numerically-stable cross-entropy.\n",
        "        Hints:\n",
        "          • Subtract max per row before exponentiation.\n",
        "          • Convert to probabilities; pick class probs at targets.\n",
        "          • Apply -log(...) and reduction (mean by default).\n",
        "          • Cache what's needed for backward.\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        shifted_logits = logits - logits.max(dim=1, keepdim=True)[0]  # (N, C)\n",
        "\n",
        "        # Compute log softmax\n",
        "        log_probs = shifted_logits - torch.log(torch.sum(torch.exp(shifted_logits), dim=1, keepdim=True))  # (N, C)\n",
        "\n",
        "        # Select log probabilities at the target indices\n",
        "        N = logits.size(0)\n",
        "        target_log_probs = log_probs[torch.arange(N), targets]  # (N,)\n",
        "\n",
        "        # Negative log-likelihood\n",
        "        loss = -target_log_probs  # (N,)\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            loss = loss.mean()\n",
        "        if self._cache is None:\n",
        "          self._cache = {}\n",
        "        # Cache for backward\n",
        "        self._cache[\"logits\"] = logits\n",
        "        self._cache[\"targets\"] = targets\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, logits=None, targets=None):\n",
        "        \"\"\"\n",
        "        TODO: Compute gradient w.r.t. logits.\n",
        "        Hints:\n",
        "          • If `logits` and `targets` are provided, recompute softmax probs stably.\n",
        "            Otherwise, use cached probs/targets from forward().\n",
        "          • Divide by batch size if reduction == 'mean'.\n",
        "        Return: grad_logits\n",
        "        \"\"\"\n",
        "        \"WRITE YOUR CODE HERE\"\n",
        "        if logits is None or targets is None:\n",
        "            logits = self._cache[\"logits\"]\n",
        "            targets = self._cache[\"targets\"]\n",
        "\n",
        "        N, C = logits.shape\n",
        "\n",
        "        # Numerical stability\n",
        "        shifted_logits = logits - logits.max(dim=1, keepdim=True)[0]\n",
        "        exp_logits = torch.exp(shifted_logits)\n",
        "        softmax = exp_logits / exp_logits.sum(dim=1, keepdim=True)  # (N, C)\n",
        "\n",
        "        # dL/dz = softmax - y_one_hot\n",
        "        grad_logits = softmax.clone()\n",
        "        grad_logits[torch.arange(N), targets] -= 1  # subtract 1 at target class\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            grad_logits /= N\n",
        "\n",
        "        return grad_logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FCQy3mhqz2v"
      },
      "source": [
        "\n",
        "## Part 2: Training the CNN\n",
        "\n",
        "Build and train the CNN for MNIST using **your custom layers**.\n",
        "\n",
        "### (a) Training pipeline code\n",
        "Address `TODO` in:\n",
        "1. **Backward pass section** — explicit chaining with your `backward()` methods.  \n",
        "2. **Parameter updates section** — manual updates using SGD-style with `torch.no_grad()`.\n",
        "\n",
        "### (b) Training and experiments\n",
        "Train your CNN on MNIST and run the following configurations:\n",
        "1. **Activations (2):** ReLU, Tanh.  \n",
        "2. **Learning rates (2):** 1e-3, 3e-4.\n",
        "\n",
        "Use consistent hyperparameters across runs. This yields 4 configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfmBUpKtrMj6"
      },
      "outputs": [],
      "source": [
        "def get_activation(name):\n",
        "    if name.lower() == \"relu\":\n",
        "        return F.relu, lambda z: (z > 0).to(z.dtype)\n",
        "    elif name.lower() == \"tanh\":\n",
        "        return torch.tanh, lambda z: 1 - torch.tanh(z)**2\n",
        "    else:\n",
        "        raise ValueError(\"Unknown activation: \" + name)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation_name=\"relu\", num_classes=10):\n",
        "        super().__init__()\n",
        "        self.ACT, self.ACT_PRIME = get_activation(activation_name)\n",
        "\n",
        "        self.conv1 = CustomConv2D(1, 32, 5)\n",
        "        self.conv2 = CustomConv2D(32, 64, 5)\n",
        "        self.max1  = CustomMaxPool2D(2, stride=2)\n",
        "        self.max2  = CustomMaxPool2D(2, stride=2)\n",
        "        self.fc1   = CustomLinear(64*4*4, 512)\n",
        "        self.fc2   = CustomLinear(512, num_classes)\n",
        "\n",
        "        self.criterion = CustomCrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.conv1(x)\n",
        "        a1 = self.ACT(z1)\n",
        "        p1 = self.max1(a1)\n",
        "        d1 = p1.clone()  # Post-pool output, if needed for backward\n",
        "\n",
        "        z2 = self.conv2(p1)\n",
        "        a2 = self.ACT(z2)\n",
        "        p2 = self.max2(a2)\n",
        "        d2 = p2.clone()  # Post-pool output, if needed for backward\n",
        "\n",
        "        flat = torch.flatten(p2, start_dim=1)\n",
        "        z3 = self.fc1(flat)\n",
        "        a3 = self.ACT(z3)\n",
        "        logits = self.fc2(a3)\n",
        "\n",
        "        # Correct 10-element cache\n",
        "        caches = (z1, a1, p1, d1, z2, a2, p2, d2, z3, a3)\n",
        "\n",
        "        return logits, caches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ez96avjrVBJ"
      },
      "source": [
        "### Data (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvbsKSkyrbjv"
      },
      "outputs": [],
      "source": [
        "\n",
        "full_train = datasets.MNIST(root=\"./data_CNN\", train=True, download=True, transform=ToTensor())\n",
        "test_ds    = datasets.MNIST(root=\"./data_CNN\", train=False, download=True, transform=ToTensor())\n",
        "\n",
        "train_size = int((1 - validation_split) * len(full_train))\n",
        "val_size   = len(full_train) - train_size\n",
        "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train/Val/Test sizes: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7eiVp0Pr-j-"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVCLP9mpsAuK"
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def accuracy_from_logits(logits, labels):\n",
        "    return (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "\n",
        "def sgd_update_(param, grad, lr):\n",
        "    param -= lr * grad\n",
        "    return param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7gkJ1mVsJbh"
      },
      "source": [
        "\n",
        "### Training Pipeline code\n",
        "Address the TODOs in the code below\n",
        "> - Complete the BACKWARD PASS (explicit chaining with your `.backward` methods).\n",
        "> - Uncomment the PARAMETER UPDATES (manual SGD with no_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3oQ2mxPtT8V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_one_config(activation_name=\"relu\", epochs=num_epochs, lr=learning_rate, num_classes=num_classes):\n",
        "    model = CNN(activation_name=activation_name, num_classes=num_classes).to(device)\n",
        "    criterion = model.criterion\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs,   val_accs   = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss, total_train_correct, total_train_examples = 0.0, 0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            # ---------- Forward ----------\n",
        "            logits, caches = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "\n",
        "            # Unpack caches (pre/post activations)\n",
        "            z1, a1, p1, d1, z2, a2, p2, d2, z3, a3 = caches\n",
        "\n",
        "            # ==========================================================================\n",
        "            # BACKWARD PASS (explicit chaining with your .backward methods)\n",
        "            # TODO: Implement the manual chain below and produce gradients with names:\n",
        "            #       grad_fc2_w, grad_fc2_b, grad_fc1_w, grad_fc1_b,\n",
        "            #       grad_conv2_w, grad_conv2_b, grad_conv1_w, grad_conv1_b\n",
        "            #\n",
        "            # Hints:\n",
        "            #   1- Check PARAMETER UPDATES codes below for ideas and variable names\n",
        "            #   2- Chain in the following way:\n",
        "            #       - dL/dlogits from Cross-Entropy Loss\n",
        "            #       - FC2 backward\n",
        "            #       - Activation after FC1\n",
        "            #       - FC1 backward\n",
        "            #       - Reshape to feature map\n",
        "            #       - MaxPool2 backward\n",
        "            #       - Activation after Conv2\n",
        "            #       - Conv2 backward\n",
        "            #       - MaxPool1 backward\n",
        "            #       - Activation after Conv1\n",
        "            #       - Conv1 backward\n",
        "            \"WRITE YOUR CODE HERE\"\n",
        "            grad_logits = criterion.backward()  # shape (N, C)\n",
        "\n",
        "            grad_fc2_out, grad_fc2_w, grad_fc2_b = model.fc2.backward(grad_logits)\n",
        "            grad_z3 = model.ACT_PRIME(z3) * grad_fc2_out\n",
        "            grad_fc1_out, grad_fc1_w, grad_fc1_b = model.fc1.backward(grad_z3)\n",
        "\n",
        "            grad_pool2_out = grad_fc1_out.view_as(p2)  # shape same as p2\n",
        "            grad_d2 = model.max2.backward(grad_pool2_out)  # max2, not pool2\n",
        "\n",
        "            grad_z2 = model.ACT_PRIME(z2) * grad_d2\n",
        "            grad_d1, grad_conv2_w, grad_conv2_b = model.conv2.backward(grad_z2)\n",
        "\n",
        "            grad_p1 = model.max1.backward(grad_d1)  # max1, not pool1\n",
        "\n",
        "            grad_z1 = model.ACT_PRIME(z1) * grad_p1\n",
        "            _, grad_conv1_w, grad_conv1_b = model.conv1.backward(grad_z1)\n",
        "\n",
        "\n",
        "            # ===========================================\n",
        "            # PARAMETER UPDATES (manual SGD with no_grad)\n",
        "            # TODO: UNCOMMENT THE CODES BELOW, DO NOT CHANGE\n",
        "            # ===========================================\n",
        "            with torch.no_grad():\n",
        "                # FC2\n",
        "                model.fc2.weight.copy_(sgd_update_(model.fc2.weight, grad_fc2_w, lr))\n",
        "                if model.fc2.bias is not None:\n",
        "                    model.fc2.bias.copy_(sgd_update_(model.fc2.bias, grad_fc2_b, lr))\n",
        "                # FC1\n",
        "                model.fc1.weight.copy_(sgd_update_(model.fc1.weight, grad_fc1_w, lr))\n",
        "                if model.fc1.bias is not None:\n",
        "                    model.fc1.bias.copy_(sgd_update_(model.fc1.bias, grad_fc1_b, lr))\n",
        "                # Conv2\n",
        "                model.conv2.weight.copy_(sgd_update_(model.conv2.weight, grad_conv2_w, lr))\n",
        "                model.conv2.bias.copy_(sgd_update_(model.conv2.bias, grad_conv2_b, lr))\n",
        "                # Conv1\n",
        "                model.conv1.weight.copy_(sgd_update_(model.conv1.weight, grad_conv1_w, lr))\n",
        "                model.conv1.bias.copy_(sgd_update_(model.conv1.bias, grad_conv1_b, lr))\n",
        "\n",
        "\n",
        "            # ---------- Train metrics ----------\n",
        "            with torch.no_grad():\n",
        "                total_train_loss += loss.item()\n",
        "                total_train_examples += yb.size(0)\n",
        "                total_train_correct += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        train_losses.append(total_train_loss / len(train_loader))\n",
        "        train_accs.append(total_train_correct / total_train_examples)\n",
        "\n",
        "        # ---------- Validation ----------\n",
        "        model.eval()\n",
        "        val_loss_sum, val_correct, val_examples = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits, _ = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_loss_sum += loss.item()\n",
        "                val_examples += yb.size(0)\n",
        "                val_correct += (logits.argmax(1) == yb).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss_sum / len(val_loader))\n",
        "        val_accs.append(val_correct / val_examples)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d} | \"\n",
        "              f\"Train Loss {train_losses[-1]:.4f}  Acc {train_accs[-1]*100:.2f}% | \"\n",
        "              f\"Val Loss {val_losses[-1]:.4f}  Acc {val_accs[-1]*100:.2f}%\")\n",
        "\n",
        "    # ---------- Test ----------\n",
        "    model.eval()\n",
        "    test_correct, test_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits, _ = model(xb)\n",
        "            test_correct += (logits.argmax(1) == yb).sum().item()\n",
        "            test_total += yb.size(0)\n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": train_losses, \"val_loss\": val_losses,\n",
        "        \"train_acc\": train_accs,   \"val_acc\": val_accs,\n",
        "        \"test_acc\": test_acc\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5cGWM7Rv1X2"
      },
      "source": [
        "### Training, and experiments\n",
        "Train your CNN on MNIST and run the following configurations:\n",
        "1. **Activations (2):** ReLU, Tanh.  \n",
        "2. **Learning rates (2):** 1e-3, 3e-4.\n",
        "\n",
        "Use consistent hyperparameters across runs. This yields 4 configurations:\n",
        "- ReLU / LR = 1e-3\n",
        "- ReLU / LR = 3e-4\n",
        "- Tanh / LR = 1e-3\n",
        "- Tanh / LR = 3e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10i1TyZowgSe"
      },
      "outputs": [],
      "source": [
        "# Training, and experiments : 4 configurations in total\n",
        "\"WRITE YOUR CODE HERE\"\n",
        "value_array = [] \n",
        "hyperparameters = [(\"relu\", 1e-3), (\"relu\", 3e-4), (\"tanh\", 1e-3), (\"tanh\", 3e-4)]\n",
        "for activation_function, learning_rate_h in hyperparameters:\n",
        "    value_array.append(train_one_config(activation_name=activation_function, epochs=num_epochs, lr=learning_rate_h, num_classes=num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqKoezEJxIXb"
      },
      "source": [
        "\n",
        "## Part 3: Results and Discussions\n",
        "\n",
        "1. **Plots:** Submit clear plots of training and validation **accuracy** and **loss** for each configuration. In addition, include the **test-set accuracy** on these plots. **Discuss results for each configuration**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSMcfgOqx-qS"
      },
      "outputs": [],
      "source": [
        "# Plots\n",
        "\"WRITE YOUR CODE HERE\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj3W2K4yxVtY"
      },
      "source": [
        "  \n",
        "2. **Write a short discussion:** (~1 page) reflecting on:\n",
        "   - Which **activations/learning rates** choices performed best and why?\n",
        "   - Any signs of over/underfitting you observed.\n",
        "\n",
        "Note: Please upload the discussion write-ups of all sections of **Part 3** as a single PDF file on Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiBviemUx4Kc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IFT6135",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
